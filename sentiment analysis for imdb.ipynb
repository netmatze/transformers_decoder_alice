{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfea9689-b9f1-430a-9c09-54d6c14b92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35879fd6-1c31-4358-9cd2-7972544364cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9479f9f-bc31-437e-9662-69a174e35dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('./datasets/IMDBDataset.csv')\n",
    "df = df.head(4096)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e8f83d-f164-4b3b-a7dc-0cdbf5ef295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        words = text.lower().split()\n",
    "        counter.update(words)\n",
    "    # Only keep words with a frequency greater than min_freq\n",
    "    vocab = {word: idx + 2 for idx, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from reviews\n",
    "vocab = build_vocab(df['review'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d51c175-0ca0-4823-a2d4-5149c5a692b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.lower().split()]\n",
    "\n",
    "df['tokenized_review'] = df['review'].apply(lambda x: tokenize(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8e2462-2ef3-4afd-b3b9-eaf046780c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.lower().split()]\n",
    "\n",
    "df['tokenized_review'] = df['review'].apply(lambda x: tokenize(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b17c907-2ee8-42ce-9822-c054dcce1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, vocab, max_length=100):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "\n",
    "        # Pad or truncate the review to max_length\n",
    "        if len(review) > self.max_length:\n",
    "            review = review[:self.max_length]\n",
    "        else:\n",
    "            review = review + [self.vocab['<PAD>']] * (self.max_length - len(review))\n",
    "\n",
    "        #print(f\"sentiment: {sentiment}\")\n",
    "\n",
    "        sentiment_value = 0\n",
    "        if sentiment == 'positive':\n",
    "            sentiment_value = 1\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(review, dtype=torch.long),\n",
    "            'sentiment': torch.tensor(sentiment_value, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3bb54f-980a-478b-9f32-84432d813e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Create the dataset\n",
    "dataset = IMDBDataset(df['tokenized_review'].tolist(), df['sentiment'].tolist(), vocab)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b9b01e-371a-49e1-997d-9a07eb9f1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens shape: torch.Size([32, 100])\n",
      "Labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    input_tokens = batch['input_ids']\n",
    "    labels = batch['sentiment']\n",
    "    print(f\"Input Tokens shape: {input_tokens.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccdd2c4d-4af5-436c-9b96-d79347e9bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:\n",
      "tensor([[   10,   540,   156,  ...,  1198,   806,  7488],\n",
      "        [  469,   249,   383,  ...,    22,   796, 36823],\n",
      "        [   12,   161,   768,  ...,  4148,    95,  3587],\n",
      "        ...,\n",
      "        [  147,   945,  1243,  ...,  9206,  1054,    54],\n",
      "        [69694, 16464,  3112,  ..., 42296,  1504,     4],\n",
      "        [  313,   705,    95,  ...,   897, 27141,    56]])\n",
      "Target Sequence:\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0])\n",
      "80921\n",
      "80921\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    input_seq = batch['input_ids']\n",
    "    target_seq = batch['sentiment']\n",
    "    \n",
    "    print(\"Input Sequence:\")\n",
    "    print(input_seq)  # (batch_size, sequence_length)\n",
    "    \n",
    "    print(\"Target Sequence:\")\n",
    "    print(target_seq)  # (batch_size,)\n",
    "    \n",
    "    # Example break to print only one batch\n",
    "    break\n",
    "\n",
    "token_to_id = dataset.vocab\n",
    "print(len(token_to_id))\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(id_to_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e43ee-e926-4b17-9846-b0ac36629326",
   "metadata": {},
   "source": [
    "## using transformer architecture for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2d48fa6-6743-4665-803e-6fc873d7ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        div_term = 1 / torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        print(word_embeddings.shape)\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        q = self.W_q(encodings)\n",
    "        k = self.W_k(encodings)\n",
    "        v = self.W_v(encodings)\n",
    "\n",
    "        # Ensure k has the same shape as q before transpose\n",
    "        assert k.shape == q.shape\n",
    "        \n",
    "        # Transpose k to align with q for dot product\n",
    "        k_transposed = k.transpose(-1, -2)\n",
    "        \n",
    "        sims = torch.matmul(q, k_transposed)\n",
    "        scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            #mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super().__init__()\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        #self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=1)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # calculate word embeddings out of the tokens\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        # apply positional encoding (using the PositionalEncoder layer) to the word embeddings\n",
    "        # position_encoded = self.pe(word_embeddings)\n",
    "        # create mask for decoder only transformer so it can not cheat\n",
    "        if (self.using_mask == True):\n",
    "            mask_ones = torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)))\n",
    "            #print(mask_ones)\n",
    "            mask = torch.tril(mask_ones)\n",
    "            #print(mask)\n",
    "            mask = mask == 0\n",
    "            #print(mask)\n",
    "            # calculate self attention with the Attention Layer\n",
    "            self_attention_values = self.self_attention(word_embeddings, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(word_embeddings)    \n",
    "        # add original position_encoded values to the calculated self attention values (residual connection)\n",
    "        # residual_connection_values = position_encoded + self_attention_values\n",
    "        # use the final linear layer to calculate the output probabilities\n",
    "        return self.fc_layer(self_attention_values.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a3b0bff1-f54b-4886-b34a-1b9c0df01174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_71740\\489849710.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.6992613198701861\n",
      "Epoch 1, Train Loss: 0.6901238225173019\n",
      "Epoch 2, Train Loss: 0.681618024650802\n",
      "Epoch 3, Train Loss: 0.6676254936918089\n",
      "Epoch 4, Train Loss: 0.6408633167635972\n",
      "Epoch 5, Train Loss: 0.5960057391279056\n",
      "Epoch 6, Train Loss: 0.5351371246234256\n",
      "Epoch 7, Train Loss: 0.46966583533747003\n",
      "Epoch 8, Train Loss: 0.40255854971097355\n",
      "Epoch 9, Train Loss: 0.3397397503748045\n",
      "Epoch 10, Train Loss: 0.281528045151551\n",
      "Epoch 11, Train Loss: 0.2287954952435907\n",
      "Epoch 12, Train Loss: 0.18438456768126787\n",
      "Epoch 13, Train Loss: 0.1468717411970117\n",
      "Epoch 14, Train Loss: 0.1155766293344512\n",
      "Epoch 15, Train Loss: 0.09001529990025638\n",
      "Epoch 16, Train Loss: 0.0702984923944878\n",
      "Epoch 17, Train Loss: 0.0547681837679559\n",
      "Epoch 18, Train Loss: 0.04235404078770197\n",
      "Epoch 19, Train Loss: 0.03257558666204397\n",
      "Epoch 20, Train Loss: 0.025428431159374616\n",
      "Epoch 21, Train Loss: 0.019941985949810174\n",
      "Epoch 22, Train Loss: 0.016098586900980736\n",
      "Epoch 23, Train Loss: 0.012808822915063186\n",
      "Epoch 24, Train Loss: 0.010343962404419024\n",
      "Epoch 25, Train Loss: 0.00849429594626698\n",
      "Epoch 26, Train Loss: 0.007150244744069122\n",
      "Epoch 27, Train Loss: 0.006029961472923011\n",
      "Epoch 28, Train Loss: 0.005160939245196858\n",
      "Epoch 29, Train Loss: 0.004449200761210565\n",
      "Epoch 30, Train Loss: 0.003760750998037857\n",
      "Epoch 31, Train Loss: 0.0033111927202886184\n",
      "Epoch 32, Train Loss: 0.0028516217990953528\n",
      "Epoch 33, Train Loss: 0.002541332744015655\n",
      "Epoch 34, Train Loss: 0.002293277475458075\n",
      "Epoch 35, Train Loss: 0.0019729737032701624\n",
      "Epoch 36, Train Loss: 0.001730174456415303\n",
      "Epoch 37, Train Loss: 0.0016060560439181534\n",
      "Epoch 38, Train Loss: 0.0014358269232174943\n",
      "Epoch 39, Train Loss: 0.0012741985859497868\n",
      "Epoch 40, Train Loss: 0.0011346313825839095\n",
      "Epoch 41, Train Loss: 0.0010288296810185803\n",
      "Epoch 42, Train Loss: 0.0009370146439397283\n",
      "Epoch 43, Train Loss: 0.0008421389753924708\n",
      "Epoch 44, Train Loss: 0.0007690092763718994\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dataset.vocab)\n",
    "print(vocab_size)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "max_len = 200\n",
    "sentiment_model = DecoderOnlyTransformer(num_tokens=vocab_size, d_model=8, max_len=max_len, using_mask=False)\n",
    "\n",
    "sentiment_model.to(device)\n",
    "optimizer = Adam(sentiment_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sentiment_model.train()\n",
    "epochs = 45\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens = data['input_ids']\n",
    "        #print(f\"input_ids: {input_tokens}\")\n",
    "        labels = data['sentiment']\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        prediction = sentiment_model(input_tokens).squeeze(1)\n",
    "        prediction = prediction[:, -1, 0]\n",
    "        labels = labels.view(-1).float()  # [batch_size * seq_length\n",
    "        #print(prediction.shape)\n",
    "        #print(prediction)\n",
    "        #print(labels.shape)\n",
    "        #print(labels)\n",
    "        loss = criterion(prediction, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8045ac4e-ba2d-479b-a413-1b2e72f35f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "tensor([ 2.6792e-01,  3.4408e-01,  2.6303e+00,  3.6051e-01,  7.4766e-01,\n",
      "         1.2125e+00, -4.2127e-01, -6.9262e-01,  4.0398e-01,  1.2544e-01,\n",
      "         6.1825e-01,  7.1614e-02,  2.0520e+00, -1.3205e+00, -7.8493e-01,\n",
      "         2.6596e-01,  2.2210e-01,  1.6068e+00, -2.4850e-04, -2.0339e-01,\n",
      "        -8.1538e-02,  5.6586e-01,  3.1171e-01,  5.5484e-01,  1.0787e+00,\n",
      "         2.7716e-01,  1.3991e-01,  1.4559e-01,  7.0210e-01,  3.1345e-01,\n",
      "         4.5697e-01,  9.2115e-01])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample tensor of shape [32, 100, 1]\n",
    "output_tensor = torch.randn(32, 100, 1)\n",
    "\n",
    "# Using the value from the last time step\n",
    "output_tensor_last_step = output_tensor[:, -1, 0]\n",
    "\n",
    "# The resulting tensor will have shape [32]\n",
    "print(output_tensor_last_step.shape)\n",
    "print(output_tensor_last_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "00d549ac-510b-4da3-a133-93b99c83431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = [\n",
    "    {\"text\": \"I love this product!\", \"label\": 1},\n",
    "    {\"text\": \"This product is terrible.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so happy with this purchase!\", \"label\": 1},\n",
    "    {\"text\": \"I regret buying this.\", \"label\": 0},\n",
    "    {\"text\": \"This is the best thing I've ever bought!\", \"label\": 1},\n",
    "    {\"text\": \"I'm disappointed with this product.\", \"label\": 0},\n",
    "    {\"text\": \"I would definitely recommend this!\", \"label\": 1},\n",
    "    {\"text\": \"This is a waste of money.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so impressed with this!\", \"label\": 1},\n",
    "    {\"text\": \"I don't like this at all.\", \"label\": 0},\n",
    "    {\"text\": \"I don't like the movie.\", \"label\": 0},\n",
    "    {\"text\": \"I love this.\", \"label\": 1},\n",
    "    {\"text\": \"This movie is bad.\", \"label\": 0},\n",
    "    {\"text\": \"This is a waste of time.\", \"label\": 0},\n",
    "    {\"text\": \"I regret looking this.\", \"label\": 0},\n",
    "    {\"text\": \"I movie is crap.\", \"label\": 0},\n",
    "    {\"text\": \"I love the movie.\", \"label\": 1},\n",
    "    {\"text\": \"The movie is a good movie.\", \"label\": 1},\n",
    "    {\"text\": \"I loved the new Marvel movie, it was so action-packed and exciting!\", \"label\": 1},\n",
    "    {\"text\": \"The latest rom-com I saw was so cheesy and predictable, I hated it.\", \"label\": 0},\n",
    "    {\"text\": \"The special effects in the new Star Wars movie were mind-blowing!\", \"label\": 1},\n",
    "    {\"text\": \"I was really disappointed with the ending of the latest Game of Thrones season.\", \"label\": 0},\n",
    "    {\"text\": \"The new Pixar movie was so heartwarming and funny.\", \"label\": 1},\n",
    "    {\"text\": \"The latest horror movie I saw was so boring.\", \"label\": 0},\n",
    "    {\"text\": \"The acting in the new biopic was incredible, the lead actor deserved an Oscar.\", \"label\": 1},\n",
    "    {\"text\": \"The plot of the latest sci-fi movie was so confusing and convoluted, I got lost.\", \"label\": 0},\n",
    "    {\"text\": \"The new comedy movie was hilarious, I laughed out loud the whole time!\", \"label\": 1},\n",
    "    {\"text\": \"The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all.\", \"label\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0cb04d55-ef62-4eb1-ba86-1f40b147dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this product!', 'This product is terrible.', \"I'm so happy with this purchase!\", 'I regret buying this.', \"This is the best thing I've ever bought!\", \"I'm disappointed with this product.\", 'I would definitely recommend this!', 'This is a waste of money.', \"I'm so impressed with this!\", \"I don't like this at all.\", \"I don't like the movie.\", 'I love this.', 'This movie is bad.', 'This is a waste of time.', 'I regret looking this.', 'I movie is crap.', 'I love the movie.', 'The movie is a good movie.', 'I loved the new Marvel movie, it was so action-packed and exciting!', 'The latest rom-com I saw was so cheesy and predictable, I hated it.', 'The special effects in the new Star Wars movie were mind-blowing!', 'I was really disappointed with the ending of the latest Game of Thrones season.', 'The new Pixar movie was so heartwarming and funny.', 'The latest horror movie I saw was so boring.', 'The acting in the new biopic was incredible, the lead actor deserved an Oscar.', 'The plot of the latest sci-fi movie was so confusing and convoluted, I got lost.', 'The new comedy movie was hilarious, I laughed out loud the whole time!', \"The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all.\"]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [sample['text'] for sample in test_datas]\n",
    "print(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b56291bd-638b-4be5-bc6e-a59ae15b0235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True,  I love this product! - test_label: 1, binary predition: 1, test_prediction: 23.75636863708496\n",
      "True,  This product is terrible. - test_label: 0, binary predition: 0, test_prediction: -31.529279708862305\n",
      "True,  I'm so happy with this purchase! - test_label: 1, binary predition: 1, test_prediction: 19.55344581604004\n",
      "True,  I regret buying this. - test_label: 0, binary predition: 0, test_prediction: -47.1557731628418\n",
      "True,  This is the best thing I've ever bought! - test_label: 1, binary predition: 1, test_prediction: 44.837825775146484\n",
      "True,  I'm disappointed with this product. - test_label: 0, binary predition: 0, test_prediction: -12.945472717285156\n",
      "False,  I would definitely recommend this! - test_label: 1, binary predition: 0, test_prediction: -23.153711318969727\n",
      "True,  This is a waste of money. - test_label: 0, binary predition: 0, test_prediction: -79.17842864990234\n",
      "False,  I'm so impressed with this! - test_label: 1, binary predition: 0, test_prediction: -19.659509658813477\n",
      "True,  I don't like this at all. - test_label: 0, binary predition: 0, test_prediction: -82.18547821044922\n",
      "True,  I don't like the movie. - test_label: 0, binary predition: 0, test_prediction: -0.2097831815481186\n",
      "True,  I love this. - test_label: 1, binary predition: 1, test_prediction: 56.468013763427734\n",
      "True,  This movie is bad. - test_label: 0, binary predition: 0, test_prediction: -45.66106033325195\n",
      "True,  This is a waste of time. - test_label: 0, binary predition: 0, test_prediction: -52.23224639892578\n",
      "True,  I regret looking this. - test_label: 0, binary predition: 0, test_prediction: -15.604586601257324\n",
      "True,  I movie is crap. - test_label: 0, binary predition: 0, test_prediction: -2.0256123542785645\n",
      "True,  I love the movie. - test_label: 1, binary predition: 1, test_prediction: 65.37895965576172\n",
      "True,  The movie is a good movie. - test_label: 1, binary predition: 1, test_prediction: 17.02661895751953\n",
      "True,  I loved the new Marvel movie, it was so action-packed and exciting! - test_label: 1, binary predition: 1, test_prediction: 32.71249008178711\n",
      "True,  The latest rom-com I saw was so cheesy and predictable, I hated it. - test_label: 0, binary predition: 0, test_prediction: -79.86050415039062\n",
      "True,  The special effects in the new Star Wars movie were mind-blowing! - test_label: 1, binary predition: 1, test_prediction: 24.1308536529541\n",
      "False,  I was really disappointed with the ending of the latest Game of Thrones season. - test_label: 0, binary predition: 1, test_prediction: 44.44639205932617\n",
      "True,  The new Pixar movie was so heartwarming and funny. - test_label: 1, binary predition: 1, test_prediction: 5.122696876525879\n",
      "True,  The latest horror movie I saw was so boring. - test_label: 0, binary predition: 0, test_prediction: -20.47647476196289\n",
      "True,  The acting in the new biopic was incredible, the lead actor deserved an Oscar. - test_label: 1, binary predition: 1, test_prediction: 48.930660247802734\n",
      "True,  The plot of the latest sci-fi movie was so confusing and convoluted, I got lost. - test_label: 0, binary predition: 0, test_prediction: -119.51435852050781\n",
      "True,  The new comedy movie was hilarious, I laughed out loud the whole time! - test_label: 1, binary predition: 1, test_prediction: 91.55272674560547\n",
      "False,  The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all. - test_label: 0, binary predition: 1, test_prediction: 2.9754648208618164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_71740\\489849710.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    }
   ],
   "source": [
    "min_len = 10\n",
    "for test_data in test_datas:\n",
    "    test_input_text = test_data['text']\n",
    "    test_label = test_data['label']\n",
    "    \n",
    "    test_indices = [vocab.get(word, vocab['<UNK>']) for word in test_input_text.lower().split()]\n",
    "    if len(test_indices) < min_len:\n",
    "        test_indices += [0] * (min_len - len(test_indices))  # Use 0 instead of self.vocab['<PAD>']\n",
    "    test_input_ids = torch.tensor(test_indices)\n",
    "\n",
    "    # Convert label to tensor\n",
    "    test_label = torch.tensor(test_label)\n",
    "\n",
    "    test_input_ids = test_input_ids.to(device)  # Move inputs to GPU if available\n",
    "    test_label = test_label.to(device)  # Move labels to GPU if available\n",
    "    #print(test_input_ids)\n",
    "    #print(test_label)\n",
    "\n",
    "    # Adding a new dimension at the beginning to change the shape to [1, 10]\n",
    "    reshaped_test_input_ids = test_input_ids.unsqueeze(0)\n",
    "\n",
    "    #print(f\"Original Tensor shape: {test_input_ids.shape}\")  # Output: torch.Size([10])\n",
    "    #print(f\"Reshaped Tensor shape: {reshaped_test_input_ids.shape}\")  # Output: torch.Size([1, 10])\n",
    "    #print(reshaped_test_input_ids)\n",
    "    \n",
    "    test_prediction = sentiment_model(reshaped_test_input_ids).squeeze(1)\n",
    "    test_prediction = test_prediction.mean()\n",
    "    threshold = 0\n",
    "    binary_prediction = (test_prediction >= threshold).long()\n",
    "    #print(f\"{test_input_text} - test_label: {test_label}\")\n",
    "    print(f\"{test_label == int(binary_prediction)},  {test_input_text} - test_label: {test_label}, binary predition: {int(binary_prediction)}, test_prediction: {test_prediction}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123d508-9607-40fd-a181-728cac7cae8b",
   "metadata": {},
   "source": [
    "## using LSTM and GRU for sentiment analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "31d4141a-906c-41eb-a0f1-a193a49e6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedding = self.embedding(text)\n",
    "        _, (hidden, _) = self.rnn(embedding)\n",
    "        #print(hidden.shape)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ba8f4a1a-77f3-4489-b3ca-5ec0a3b1a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedding = self.embedding(text)\n",
    "        _, hidden = self.rnn(embedding)\n",
    "        #print(hidden.shape)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ca5f4-308b-4bbc-b724-513040827126",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dataset.vocab)\n",
    "print(vocab_size)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "\n",
    "#sentiment_model = SentimentModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "#sentiment_model = SentimentRnnModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "#sentiment_model = SentimentLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "sentiment_model = SentimentGRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "sentiment_model.to(device)\n",
    "optimizer = Adam(sentiment_model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sentiment_model.train()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens = data['input_ids']\n",
    "        #print(f\"input_ids: {input_tokens}\")\n",
    "        labels = data['sentiment']\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        prediction = sentiment_model(input_tokens).squeeze(1)\n",
    "        labels = labels.view(-1).float()  # [batch_size * seq_length\n",
    "        #print(prediction.shape)\n",
    "        #print(prediction)\n",
    "        #print(labels.shape)\n",
    "        #print(labels)\n",
    "        loss = criterion(prediction, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c70c1958-ae69-4666-bdf4-4e532cb17095",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = [\n",
    "    {\"text\": \"I love this product!\", \"label\": 1},\n",
    "    {\"text\": \"This product is terrible.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so happy with this purchase!\", \"label\": 1},\n",
    "    {\"text\": \"I regret buying this.\", \"label\": 0},\n",
    "    {\"text\": \"This is the best thing I've ever bought!\", \"label\": 1},\n",
    "    {\"text\": \"I'm disappointed with this product.\", \"label\": 0},\n",
    "    {\"text\": \"I would definitely recommend this!\", \"label\": 1},\n",
    "    {\"text\": \"This is a waste of money.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so impressed with this!\", \"label\": 1},\n",
    "    {\"text\": \"I don't like this at all.\", \"label\": 0},\n",
    "    {\"text\": \"I don't like it.\", \"label\": 0},\n",
    "    {\"text\": \"I love this.\", \"label\": 1},\n",
    "    {\"text\": \"This movie is bad.\", \"label\": 0},\n",
    "    {\"text\": \"This is a waste of time.\", \"label\": 0},\n",
    "    {\"text\": \"I regret looking this.\", \"label\": 0},\n",
    "    {\"text\": \"I movie is crap.\", \"label\": 0},\n",
    "    {\"text\": \"I love the movie.\", \"label\": 1},\n",
    "    {\"text\": \"The movie is a good movie.\", \"label\": 1},\n",
    "    {\"text\": \"I loved the new Marvel movie, it was so action-packed and exciting!\", \"label\": 1},\n",
    "    {\"text\": \"The latest rom-com I saw was so cheesy and predictable, I hated it.\", \"label\": 0},\n",
    "    {\"text\": \"The special effects in the new Star Wars movie were mind-blowing, I was on the edge of my seat!\", \"label\": 1},\n",
    "    {\"text\": \"I was really disappointed with the ending of the latest Game of Thrones season, it was so rushed.\", \"label\": 0},\n",
    "    {\"text\": \"The new Pixar movie was so heartwarming and funny, I cried and laughed at the same time!\", \"label\": 1},\n",
    "    {\"text\": \"The latest horror movie I saw was so boring and not scary at all, I fell asleep.\", \"label\": 0},\n",
    "    {\"text\": \"The acting in the new biopic was incredible, the lead actor deserved an Oscar.\", \"label\": 1},\n",
    "    {\"text\": \"The plot of the latest sci-fi movie was so confusing and convoluted, I got lost.\", \"label\": 0},\n",
    "    {\"text\": \"The new comedy movie was hilarious, I laughed out loud the whole time!\", \"label\": 1},\n",
    "    {\"text\": \"The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all.\", \"label\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aafdbc6d-d996-4ebb-bf10-06888fc05cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_71740\\1014245992.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False,  I love this product! - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  This product is terrible. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I'm so happy with this purchase! - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I regret buying this. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  This is the best thing I've ever bought! - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I'm disappointed with this product. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I would definitely recommend this! - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  This is a waste of money. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I'm so impressed with this! - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I don't like this at all. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I don't like it. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I love this. - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  This movie is bad. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  This is a waste of time. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I regret looking this. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "True,  I movie is crap. - test_label: 0, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I love the movie. - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  The movie is a good movie. - test_label: 1, binary predition: 0, test_prediction: -8.277002780232579e-05\n",
      "False,  I loved the new Marvel movie, it was so action-packed and exciting! - test_label: 1, binary predition: 0, test_prediction: -8.277001325041056e-05\n",
      "True,  The latest rom-com I saw was so cheesy and predictable, I hated it. - test_label: 0, binary predition: 0, test_prediction: -8.277002052636817e-05\n",
      "False,  The special effects in the new Star Wars movie were mind-blowing, I was on the edge of my seat! - test_label: 1, binary predition: 0, test_prediction: -8.277001325041056e-05\n",
      "True,  I was really disappointed with the ending of the latest Game of Thrones season, it was so rushed. - test_label: 0, binary predition: 0, test_prediction: -8.277001325041056e-05\n",
      "False,  The new Pixar movie was so heartwarming and funny, I cried and laughed at the same time! - test_label: 1, binary predition: 0, test_prediction: -8.277002052636817e-05\n",
      "True,  The latest horror movie I saw was so boring and not scary at all, I fell asleep. - test_label: 0, binary predition: 0, test_prediction: -8.277002052636817e-05\n",
      "False,  The acting in the new biopic was incredible, the lead actor deserved an Oscar. - test_label: 1, binary predition: 0, test_prediction: -8.277001325041056e-05\n",
      "True,  The plot of the latest sci-fi movie was so confusing and convoluted, I got lost. - test_label: 0, binary predition: 0, test_prediction: -8.277002052636817e-05\n",
      "False,  The new comedy movie was hilarious, I laughed out loud the whole time! - test_label: 1, binary predition: 0, test_prediction: -8.277002052636817e-05\n",
      "True,  The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all. - test_label: 0, binary predition: 0, test_prediction: -8.277002052636817e-05\n"
     ]
    }
   ],
   "source": [
    "min_len = 10\n",
    "for test_data in test_datas:\n",
    "    test_input_text = test_data['text']\n",
    "    test_label = test_data['label']\n",
    "    \n",
    "    test_indices = [vocab.get(word, vocab['<UNK>']) for word in test_input_text.lower().split()]\n",
    "    if len(test_indices) < min_len:\n",
    "        test_indices += [0] * (min_len - len(test_indices))  # Use 0 instead of self.vocab['<PAD>']\n",
    "    test_input_ids = torch.tensor(test_indices)\n",
    "\n",
    "    # Convert label to tensor\n",
    "    test_label = torch.tensor(test_label)\n",
    "\n",
    "    test_input_ids = test_input_ids.to(device)  # Move inputs to GPU if available\n",
    "    test_label = test_label.to(device)  # Move labels to GPU if available\n",
    "    #print(test_input_ids)\n",
    "    #print(test_label)\n",
    "\n",
    "    # Adding a new dimension at the beginning to change the shape to [1, 10]\n",
    "    reshaped_test_input_ids = test_input_ids.unsqueeze(0)\n",
    "\n",
    "    #print(f\"Original Tensor shape: {test_input_ids.shape}\")  # Output: torch.Size([10])\n",
    "    #print(f\"Reshaped Tensor shape: {reshaped_test_input_ids.shape}\")  # Output: torch.Size([1, 10])\n",
    "    #print(reshaped_test_input_ids)\n",
    "    \n",
    "    test_prediction = sentiment_model(reshaped_test_input_ids).squeeze(1)\n",
    "    test_prediction = test_prediction.mean()\n",
    "    threshold = 0\n",
    "    binary_prediction = (test_prediction >= threshold).long()\n",
    "    print(f\"{test_label == int(binary_prediction)},  {test_input_text} - test_label: {test_label}, binary predition: {int(binary_prediction)}, test_prediction: {test_prediction}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6bdb3-9778-4999-8fb4-cfda246e008b",
   "metadata": {},
   "source": [
    "## using decoder only transfomer for sentiment analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7a86acf4-f75b-4e29-9b00-ddd8f8204414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        div_term = 1 / torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        #print(div_term)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        pe_temp = self.pe[:word_embeddings.size(0), :]\n",
    "        pe_temp_expanded = pe_temp.unsqueeze(1)\n",
    "        #print(f\"word_embeddings.shape: {word_embeddings.shape}, self.pe.shape: {pe_temp_expanded.shape}, \")\n",
    "        return word_embeddings + pe_temp_expanded\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        q = self.W_q(encodings)\n",
    "        k = self.W_k(encodings)\n",
    "        v = self.W_v(encodings)\n",
    "\n",
    "        # Ensure k has the same shape as q before transpose\n",
    "        assert k.shape == q.shape\n",
    "\n",
    "        # Transpose k to align with q for dot product\n",
    "        k_transposed = k.transpose(-1, -2)\n",
    "\n",
    "        # Check shapes\n",
    "        #print(\"Shape of q:\", q.shape)  # [1, 5, 2]\n",
    "        #print(\"Shape of k_transposed:\", k_transposed.shape)  # [1, 2, 5]\n",
    "        sims = torch.matmul(q, k_transposed)\n",
    "        scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=2,heads=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_qs = []\n",
    "        self.W_ks = []\n",
    "        self.W_vs = []\n",
    "\n",
    "        for index in range(heads):\n",
    "            W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "            self.W_qs.append(W_q)\n",
    "            self.W_ks.append(W_k)\n",
    "            self.W_vs.append(W_v)\n",
    "\n",
    "        self.unify_heads = nn.Linear(d_model * heads, d_model)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        attentionscores = []\n",
    "        #encodings.to(device)\n",
    "        for index in range(self.heads):\n",
    "            W_q = self.W_qs[index].to(device)\n",
    "            W_k = self.W_ks[index].to(device)\n",
    "            W_v = self.W_vs[index].to(device)\n",
    "\n",
    "            q = W_q(encodings.to(device))\n",
    "            k = W_k(encodings.to(device))\n",
    "            v = W_v(encodings.to(device))\n",
    "\n",
    "            k_transposed = k.transpose(-1, -2)\n",
    "            sims = torch.matmul(q, k_transposed)\n",
    "            scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "                scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "            attention_percents = F.softmax(scaled_sims)\n",
    "            attention_scores = torch.matmul(attention_percents, v)\n",
    "            attentionscores.append(attention_scores)\n",
    "\n",
    "        combined_attention_scores = torch.cat(attentionscores, dim=-1)\n",
    "        combined_output = self.unify_heads(combined_attention_scores)\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_tokens, using_mask=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, heads=num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.fc_layer2 = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        dropout=0.1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, position_encoded, mask=None):\n",
    "        if self.using_mask:\n",
    "            self_attention_values = self.self_attention(position_encoded, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(position_encoded)\n",
    "\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        normalized_values1 = self.layer_norm1(residual_connection_values)\n",
    "\n",
    "        fc_layer_output_relu = self.relu(self.fc_layer(normalized_values1))\n",
    "        #fc_layer_output_dropout = self.dropout(fc_layer_output_relu)\n",
    "        #fc_layer_output = self.fc_layer2(fc_layer_output_dropout)\n",
    "        #final_output = self.layer_norm2(normalized_values1 + fc_layer_output)\n",
    "        #fc_layer_output = self.fc_layer2(self.dropout(self.relu(self.fc_layer(normalized_values1))))\n",
    "        #return final_output\n",
    "        #fc_layer_output = self.fc_layer(normalized_values1)\n",
    "        fc_layer_output = self.relu(self.fc_layer(normalized_values1))\n",
    "        return fc_layer_output\n",
    "\n",
    "class DecoderOnlyTransformerBlockTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super(DecoderOnlyTransformerBlockTransformer, self).__init__()\n",
    "        self.number_heads = 4\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.decoder_block1 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block2 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block3 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block4 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block5 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block6 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        if self.decoder_block1.using_mask:\n",
    "            mask_ones = torch.ones((token_ids.size(dim=1), token_ids.size(dim=1)))\n",
    "            mask = torch.tril(mask_ones)\n",
    "            mask = mask == 0\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        output_block1 = self.decoder_block1(position_encoded, mask=mask)\n",
    "        #output_block2 = self.decoder_block2(output_block1, mask=mask)\n",
    "        #output_block3 = self.decoder_block3(output_block2, mask=mask)\n",
    "        #output_block4 = self.decoder_block4(output_block3, mask=mask)\n",
    "        #output_block5 = self.decoder_block5(output_block4, mask=mask)\n",
    "        #output_block6 = self.decoder_block6(output_block5, mask=mask)\n",
    "\n",
    "        fc_layer_output = self.fc_layer(output_block1)\n",
    "\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2e30a1a8-09ea-42d3-9020-641580ca4665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_71740\\2221277640.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.7029317706204742\n",
      "Epoch 1, Train Loss: 0.6932171292066283\n",
      "Epoch 2, Train Loss: 0.6807057850235694\n",
      "Epoch 3, Train Loss: 0.5996656423668867\n",
      "Epoch 4, Train Loss: 0.4255859572372157\n",
      "Epoch 5, Train Loss: 0.21689187275912153\n",
      "Epoch 6, Train Loss: 0.1025290111118475\n",
      "Epoch 7, Train Loss: 0.0818839395721244\n",
      "Epoch 8, Train Loss: 0.0294594296296292\n",
      "Epoch 9, Train Loss: 0.021285514667646117\n",
      "Epoch 10, Train Loss: 0.013430541948675745\n",
      "Epoch 11, Train Loss: 0.013425597444709569\n",
      "Epoch 12, Train Loss: 0.0017188184128193852\n",
      "Epoch 13, Train Loss: 0.011529531356486548\n",
      "Epoch 14, Train Loss: 0.007095039225910241\n",
      "Epoch 15, Train Loss: 0.00317407891453714\n",
      "Epoch 16, Train Loss: 0.003044310553399937\n",
      "Epoch 17, Train Loss: 0.0039697438784255515\n",
      "Epoch 18, Train Loss: 0.004232281420894333\n",
      "Epoch 19, Train Loss: 0.0011917188496165395\n",
      "Epoch 20, Train Loss: 0.00028811999287639274\n",
      "Epoch 21, Train Loss: 0.00024084853033931255\n",
      "Epoch 22, Train Loss: 0.013255235374079513\n",
      "Epoch 23, Train Loss: 0.00449025483512675\n",
      "Epoch 24, Train Loss: 0.013648102032198012\n",
      "Epoch 25, Train Loss: 0.003218702198631572\n",
      "Epoch 26, Train Loss: 0.0024256401613431005\n",
      "Epoch 27, Train Loss: 4.721105596016968e-05\n",
      "Epoch 28, Train Loss: 3.69734789162885e-05\n",
      "Epoch 29, Train Loss: 2.527957223355326e-05\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dataset.vocab)\n",
    "print(vocab_size)\n",
    "embedding_dim = 100\n",
    "d_model = 256\n",
    "output_dim = 1\n",
    "max_len = 100\n",
    "sentiment_model = DecoderOnlyTransformerBlockTransformer(num_tokens=vocab_size, d_model=d_model, max_len=max_len, using_mask=False)\n",
    "\n",
    "sentiment_model.to(device)\n",
    "optimizer = Adam(sentiment_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sentiment_model.train()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens = data['input_ids']\n",
    "        #print(f\"input_ids: {input_tokens}\")\n",
    "        labels = data['sentiment']\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        prediction = sentiment_model(input_tokens).squeeze(1)\n",
    "        prediction = prediction[:, -1, 0]\n",
    "        labels = labels.view(-1).float()  # [batch_size * seq_length\n",
    "        #print(prediction.shape)\n",
    "        #print(prediction)\n",
    "        #print(labels.shape)\n",
    "        #print(labels)\n",
    "        loss = criterion(prediction, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e97e8450-0b8a-4935-847b-8d6749825e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = [\n",
    "    {\"text\": \"I love this product!\", \"label\": 1},\n",
    "    {\"text\": \"This product is terrible.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so happy with this purchase!\", \"label\": 1},\n",
    "    {\"text\": \"I regret buying this.\", \"label\": 0},\n",
    "    {\"text\": \"This is the best thing I've ever bought!\", \"label\": 1},\n",
    "    {\"text\": \"I'm disappointed with this product.\", \"label\": 0},\n",
    "    {\"text\": \"I would definitely recommend this!\", \"label\": 1},\n",
    "    {\"text\": \"This is a waste of money.\", \"label\": 0},\n",
    "    {\"text\": \"I'm so impressed with this!\", \"label\": 1},\n",
    "    {\"text\": \"I don't like this at all.\", \"label\": 0},\n",
    "    {\"text\": \"I don't like the movie.\", \"label\": 0},\n",
    "    {\"text\": \"I love this.\", \"label\": 1},\n",
    "    {\"text\": \"This movie is bad.\", \"label\": 0},\n",
    "    {\"text\": \"This is a waste of time.\", \"label\": 0},\n",
    "    {\"text\": \"I regret looking this.\", \"label\": 0},\n",
    "    {\"text\": \"I movie is crap.\", \"label\": 0},\n",
    "    {\"text\": \"I love the movie.\", \"label\": 1},\n",
    "    {\"text\": \"The movie is a good movie.\", \"label\": 1},\n",
    "    {\"text\": \"I loved the new Marvel movie, it was so action-packed and exciting!\", \"label\": 1},\n",
    "    {\"text\": \"The latest rom-com I saw was so cheesy and predictable, I hated it.\", \"label\": 0},\n",
    "    {\"text\": \"The special effects in the new Star Wars movie were mind-blowing!\", \"label\": 1},\n",
    "    {\"text\": \"I was really disappointed with the ending of the latest Game of Thrones season.\", \"label\": 0},\n",
    "    {\"text\": \"The new Pixar movie was so heartwarming and funny.\", \"label\": 1},\n",
    "    {\"text\": \"The latest horror movie I saw was so boring.\", \"label\": 0},\n",
    "    {\"text\": \"The acting in the new biopic was incredible, the lead actor deserved an Oscar.\", \"label\": 1},\n",
    "    {\"text\": \"The plot of the latest sci-fi movie was so confusing and convoluted, I got lost.\", \"label\": 0},\n",
    "    {\"text\": \"The new comedy movie was hilarious, I laughed out loud the whole time!\", \"label\": 1},\n",
    "    {\"text\": \"The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all.\", \"label\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fcf42a63-c08c-454a-9559-9975c342de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_71740\\2221277640.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True,  I love this product! - test_label: 1, binary predition: 1, test_prediction: 0.000282477616565302\n",
      "True,  This product is terrible. - test_label: 0, binary predition: 0, test_prediction: -0.0010507262777537107\n",
      "False,  I'm so happy with this purchase! - test_label: 1, binary predition: 0, test_prediction: -0.00031125196255743504\n",
      "True,  I regret buying this. - test_label: 0, binary predition: 0, test_prediction: -0.00023693035473115742\n",
      "True,  This is the best thing I've ever bought! - test_label: 1, binary predition: 1, test_prediction: 0.0006155656883493066\n",
      "True,  I'm disappointed with this product. - test_label: 0, binary predition: 0, test_prediction: -0.0018173090647906065\n",
      "True,  I would definitely recommend this! - test_label: 1, binary predition: 1, test_prediction: 0.0005897162482142448\n",
      "True,  This is a waste of money. - test_label: 0, binary predition: 0, test_prediction: -0.0020566012244671583\n",
      "False,  I'm so impressed with this! - test_label: 1, binary predition: 0, test_prediction: -0.00047288116184063256\n",
      "True,  I don't like this at all. - test_label: 0, binary predition: 0, test_prediction: -0.002302520675584674\n",
      "True,  I don't like the movie. - test_label: 0, binary predition: 0, test_prediction: -0.0008920934633351862\n",
      "True,  I love this. - test_label: 1, binary predition: 1, test_prediction: 0.0006467569619417191\n",
      "True,  This movie is bad. - test_label: 0, binary predition: 0, test_prediction: -0.0012573697604238987\n",
      "True,  This is a waste of time. - test_label: 0, binary predition: 0, test_prediction: -0.00048230658285319805\n",
      "False,  I regret looking this. - test_label: 0, binary predition: 1, test_prediction: 0.000155723188072443\n",
      "False,  I movie is crap. - test_label: 0, binary predition: 1, test_prediction: 0.0006424178718589246\n",
      "True,  I love the movie. - test_label: 1, binary predition: 1, test_prediction: 0.0005667110090143979\n",
      "True,  The movie is a good movie. - test_label: 1, binary predition: 1, test_prediction: 0.0007128416909836233\n",
      "True,  I loved the new Marvel movie, it was so action-packed and exciting! - test_label: 1, binary predition: 1, test_prediction: 0.0005944097065366805\n",
      "True,  The latest rom-com I saw was so cheesy and predictable, I hated it. - test_label: 0, binary predition: 0, test_prediction: -0.0029875889886170626\n",
      "False,  The special effects in the new Star Wars movie were mind-blowing! - test_label: 1, binary predition: 0, test_prediction: -0.002723987912759185\n",
      "False,  I was really disappointed with the ending of the latest Game of Thrones season. - test_label: 0, binary predition: 1, test_prediction: 0.0005402262904681265\n",
      "False,  The new Pixar movie was so heartwarming and funny. - test_label: 1, binary predition: 0, test_prediction: -0.002225155010819435\n",
      "True,  The latest horror movie I saw was so boring. - test_label: 0, binary predition: 0, test_prediction: -0.003182558808475733\n",
      "False,  The acting in the new biopic was incredible, the lead actor deserved an Oscar. - test_label: 1, binary predition: 0, test_prediction: -0.003208214184269309\n",
      "True,  The plot of the latest sci-fi movie was so confusing and convoluted, I got lost. - test_label: 0, binary predition: 0, test_prediction: -0.0014867284335196018\n",
      "False,  The new comedy movie was hilarious, I laughed out loud the whole time! - test_label: 1, binary predition: 0, test_prediction: -0.0004065968678332865\n",
      "True,  The latest drama movie I saw was so depressing and slow, I didn't enjoy it at all. - test_label: 0, binary predition: 0, test_prediction: -0.0017050227615982294\n"
     ]
    }
   ],
   "source": [
    "min_len = 10\n",
    "sentiment_model.eval()\n",
    "for test_data in test_datas:\n",
    "    test_input_text = test_data['text']\n",
    "    test_label = test_data['label']\n",
    "    \n",
    "    test_indices = [vocab.get(word, vocab['<UNK>']) for word in test_input_text.lower().split()]\n",
    "    if len(test_indices) < min_len:\n",
    "        test_indices += [0] * (min_len - len(test_indices))  # Use 0 instead of self.vocab['<PAD>']\n",
    "    test_input_ids = torch.tensor(test_indices)\n",
    "\n",
    "    # Convert label to tensor\n",
    "    test_label = torch.tensor(test_label)\n",
    "\n",
    "    test_input_ids = test_input_ids.to(device)  # Move inputs to GPU if available\n",
    "    test_label = test_label.to(device)  # Move labels to GPU if available\n",
    "    #print(test_input_ids)\n",
    "    #print(test_label)\n",
    "\n",
    "    # Adding a new dimension at the beginning to change the shape to [1, 10]\n",
    "    reshaped_test_input_ids = test_input_ids.unsqueeze(0)\n",
    "\n",
    "    #print(f\"Original Tensor shape: {test_input_ids.shape}\")  # Output: torch.Size([10])\n",
    "    #print(f\"Reshaped Tensor shape: {reshaped_test_input_ids.shape}\")  # Output: torch.Size([1, 10])\n",
    "    #print(reshaped_test_input_ids)\n",
    "    \n",
    "    test_prediction = sentiment_model(reshaped_test_input_ids).squeeze(1)\n",
    "    test_prediction = test_prediction.mean()\n",
    "    threshold = 0\n",
    "    binary_prediction = (test_prediction >= threshold).long()\n",
    "    print(f\"{test_label == int(binary_prediction)},  {test_input_text} - test_label: {test_label}, binary predition: {int(binary_prediction)}, test_prediction: {test_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024896c2-8846-404c-b7a9-5b72933a56d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2884b87-2380-447c-ab08-953158a4a86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0071cb-2e53-47d9-b828-55bd105db250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
