{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58dd31a0-5454-4dbb-b674-63d9e6fc7a3a",
   "metadata": {},
   "source": [
    "## creating a gpt2 like decoder only transformer using the verdict by Edith Wharton\n",
    "\n",
    "### https://en.wikisource.org/wiki/The_Verdict\n",
    "\n",
    "### https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aae8be0-32fb-4f5c-be7b-0226fd62834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.0+cu121\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28df27f7-30fa-4be1-9b08-41eebe743576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc62be0-8560-4eaf-b011-730d4b7aea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8afdd58-98f0-4d41-85c5-ea26057f3d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba6594d-12e9-410d-9444-acaa00ccb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cc1e64-53bd-457a-b92b-2430c4b598b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217d7407-ecb9-4305-b778-a1cf45529268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e1018e-a2d7-4942-9fb5-945444b49123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf5e26c-e1d2-4060-955c-339f701890d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d07289a-ce0f-40f0-b346-098ab829dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef4eb77-6983-4287-8bda-bb4b5f331807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fab72c8-f87b-4a90-bbf6-0be3c23d8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "193d977b-5548-409d-b937-1d0d4e9cbfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_chunk [3198, 4930, 7683, 6675] - One Two Three Four\n",
      "target_chunk [4930, 7683, 6675, 10579] -  Two Three Four Five\n",
      "input_chunk [4930, 7683, 6675, 10579] -  Two Three Four Five\n",
      "target_chunk [7683, 6675, 10579, 9699] -  Three Four Five Six\n",
      "input_chunk [7683, 6675, 10579, 9699] -  Three Four Five Six\n",
      "target_chunk [6675, 10579, 9699, 3598] -  Four Five Six seven\n",
      "input_chunk [6675, 10579, 9699, 3598] -  Four Five Six seven\n",
      "target_chunk [10579, 9699, 3598, 3624] -  Five Six seven eight\n",
      "input_chunk [10579, 9699, 3598, 3624] -  Five Six seven eight\n",
      "target_chunk [9699, 3598, 3624, 5193] -  Six seven eight nine\n",
      "input_chunk [9699, 3598, 3624, 5193] -  Six seven eight nine\n",
      "target_chunk [3598, 3624, 5193, 3478] -  seven eight nine ten\n",
      "input_chunk [3598, 3624, 5193, 3478] -  seven eight nine ten\n",
      "target_chunk [3624, 5193, 3478, 22216] -  eight nine ten eleven\n",
      "input_chunk [3624, 5193, 3478, 22216] -  eight nine ten eleven\n",
      "target_chunk [5193, 3478, 22216, 14104] -  nine ten eleven twelve\n",
      "<class '__main__.GPTDatasetV1'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "max_length = 4\n",
    "stride=1\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):            \n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            input_chunk_text = tokenizer.decode(input_chunk)\n",
    "            print(f\"input_chunk {input_chunk} - {input_chunk_text}\")            \n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            target_chunk_text = tokenizer.decode(target_chunk)\n",
    "            print(f\"target_chunk {target_chunk} - {target_chunk_text}\")\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            if i > 10:\n",
    "                break\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "text = \"One Two Three Four Five Six seven eight nine ten eleven twelve\"\n",
    "dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b42d28db-5422-4caa-a5c7-a6e824c4ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000025725C66070>\n",
      "Batch 0:\n",
      "Input IDs: tensor([[ 3198,  4930,  7683,  6675],\n",
      "        [ 4930,  7683,  6675, 10579]]) - ['One Two Three Four', ' Two Three Four Five']\n",
      "Target IDs: tensor([[ 4930,  7683,  6675, 10579],\n",
      "        [ 7683,  6675, 10579,  9699]]) - [' Two Three Four Five', ' Three Four Five Six']\n",
      "Batch 1:\n",
      "Input IDs: tensor([[ 7683,  6675, 10579,  9699],\n",
      "        [ 6675, 10579,  9699,  3598]]) - [' Three Four Five Six', ' Four Five Six seven']\n",
      "Target IDs: tensor([[ 6675, 10579,  9699,  3598],\n",
      "        [10579,  9699,  3598,  3624]]) - [' Four Five Six seven', ' Five Six seven eight']\n",
      "Batch 2:\n",
      "Input IDs: tensor([[10579,  9699,  3598,  3624],\n",
      "        [ 9699,  3598,  3624,  5193]]) - [' Five Six seven eight', ' Six seven eight nine']\n",
      "Target IDs: tensor([[9699, 3598, 3624, 5193],\n",
      "        [3598, 3624, 5193, 3478]]) - [' Six seven eight nine', ' seven eight nine ten']\n",
      "Batch 3:\n",
      "Input IDs: tensor([[ 3598,  3624,  5193,  3478],\n",
      "        [ 3624,  5193,  3478, 22216]]) - [' seven eight nine ten', ' eight nine ten eleven']\n",
      "Target IDs: tensor([[ 3624,  5193,  3478, 22216],\n",
      "        [ 5193,  3478, 22216, 14104]]) - [' eight nine ten eleven', ' nine ten eleven twelve']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2  # Adjust the batch size as needed\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "print(data_loader)\n",
    "\n",
    "for batch_idx, (input_ids, target_ids) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    input_chunk_texts = []\n",
    "    \n",
    "    # input_ids is a batch of tensors, so you need to iterate over them\n",
    "    for id_tensor in input_ids:\n",
    "        # Convert the tensor to a list of integers\n",
    "        id_list = id_tensor.tolist()\n",
    "        # Decode the list of token ids to text\n",
    "        input_chunk_text = tokenizer.decode(id_list)\n",
    "        input_chunk_texts.append(input_chunk_text)\n",
    "    \n",
    "    print(f\"Input IDs: {input_ids} - {input_chunk_texts}\")\n",
    "    \n",
    "    # Similarly, decode target_ids\n",
    "    target_chunk_texts = []\n",
    "    for target_tensor in target_ids:\n",
    "        target_list = target_tensor.tolist()\n",
    "        target_chunk_text = tokenizer.decode(target_list)\n",
    "        target_chunk_texts.append(target_chunk_text)\n",
    "    \n",
    "    print(f\"Target IDs: {target_ids} - {target_chunk_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4991cbc-9384-4fdc-889a-498f43729088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "max_length = 4\n",
    "stride = 1\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):            \n",
    "            input_chunk = token_ids[i : i + max_length]           \n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]         \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, max_length, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5ffeb79-6569-4cea-9c59-3dbcd061888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID 0: tensor([  40,  367, 2885, 1464]) - I HAD always\n",
      "Target ID 0: tensor([ 367, 2885, 1464, 1807]) -  HAD always thought\n",
      "Input ID 1: tensor([ 367, 2885, 1464, 1807]) -  HAD always thought\n",
      "Target ID 1: tensor([2885, 1464, 1807, 3619]) - AD always thought Jack\n",
      "Input ID 2: tensor([2885, 1464, 1807, 3619]) - AD always thought Jack\n",
      "Target ID 2: tensor([1464, 1807, 3619,  402]) -  always thought Jack G\n",
      "Input ID 3: tensor([1464, 1807, 3619,  402]) -  always thought Jack G\n",
      "Target ID 3: tensor([1807, 3619,  402,  271]) -  thought Jack Gis\n",
      "Input ID 4: tensor([1807, 3619,  402,  271]) -  thought Jack Gis\n",
      "Target ID 4: tensor([ 3619,   402,   271, 10899]) -  Jack Gisburn\n",
      "Input ID 5: tensor([ 3619,   402,   271, 10899]) -  Jack Gisburn\n",
      "Target ID 5: tensor([  402,   271, 10899,  2138]) -  Gisburn rather\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1  # Adjust the batch size as needed\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Iterate over the first 6 items\n",
    "for i in range(6):        \n",
    "    # Get the next batch (input_ids, target_ids)\n",
    "    item_batch = next(data_iter)\n",
    "    \n",
    "    # item_batch is a tuple (input_ids, target_ids)\n",
    "    input_ids, target_ids = item_batch\n",
    "    \n",
    "    # Since batch_size is 1, input_ids and target_ids will have a single element each\n",
    "    input_id = input_ids[0]\n",
    "    target_id = target_ids[0]\n",
    "    \n",
    "    # Convert tensors to lists of token IDs\n",
    "    input_id_list = input_id.tolist()\n",
    "    target_id_list = target_id.tolist()\n",
    "    \n",
    "    # Decode the token IDs to text\n",
    "    input_chunk_text = tokenizer.decode(input_id_list)\n",
    "    target_chunk_text = tokenizer.decode(target_id_list)\n",
    "    \n",
    "    # Print the input and target IDs as well as their decoded text\n",
    "    print(f\"Input ID {i}: {input_id} - {input_chunk_text}\")    \n",
    "    print(f\"Target ID {i}: {target_id} - {target_chunk_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16162b08-6429-41b5-8ee8-6cec71d00977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 4\n",
    "stride = 4\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, max_length, stride)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "data_iter = iter(data_loader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68a5ad14-c356-4853-aa10-39433542c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(torch.arange(context_length))\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "#print(pos_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59372895-2871-405f-88ac-dbb51f25d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eea6ac-c667-4be5-94f2-45fdf935e503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
