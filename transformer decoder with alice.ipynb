{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d438488f-b9ca-432c-8e6a-b7efb1ce50fc",
   "metadata": {},
   "source": [
    "## Transformers decoder only (gpt2 like) trained at Lewis Carrolls Alice's Adventures in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21e1e00-f3d9-4d5c-b112-76c6cde83584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./content/drive/MyDrive/datasets/alice.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#import lightning as L\n",
    "\n",
    "\"./content/drive/MyDrive/datasets/alice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9c1ec1-0f76-417f-83cc-2ad0c6728cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a3eaab-b15f-4d2b-a275-3a883cb07817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        div_term = 1 / torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        #print(div_term)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        pe_temp = self.pe[:word_embeddings.size(0), :]\n",
    "        pe_temp_expanded = pe_temp.unsqueeze(1)\n",
    "        #print(f\"word_embeddings.shape: {word_embeddings.shape}, self.pe.shape: {pe_temp_expanded.shape}, \")\n",
    "        return word_embeddings + pe_temp_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ef9c2c-f4b7-4bcc-a410-f4b74e67e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        q = self.W_q(encodings)\n",
    "        k = self.W_k(encodings)\n",
    "        v = self.W_v(encodings)\n",
    "\n",
    "        # Ensure k has the same shape as q before transpose\n",
    "        assert k.shape == q.shape\n",
    "\n",
    "        # Transpose k to align with q for dot product\n",
    "        k_transposed = k.transpose(-1, -2)\n",
    "\n",
    "        # Check shapes\n",
    "        #print(\"Shape of q:\", q.shape)  # [1, 5, 2]\n",
    "        #print(\"Shape of k_transposed:\", k_transposed.shape)  # [1, 2, 5]\n",
    "        sims = torch.matmul(q, k_transposed)\n",
    "        scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23aac6e2-bfce-4ed4-91f0-b344ce83399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=2,heads=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_qs = []\n",
    "        self.W_ks = []\n",
    "        self.W_vs = []\n",
    "\n",
    "        for index in range(heads):\n",
    "            W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "            self.W_qs.append(W_q)\n",
    "            self.W_ks.append(W_k)\n",
    "            self.W_vs.append(W_v)\n",
    "\n",
    "        self.unify_heads = nn.Linear(d_model * heads, d_model)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        attentionscores = []\n",
    "        #encodings.to(device)\n",
    "        for index in range(self.heads):\n",
    "            W_q = self.W_qs[index].to(device)\n",
    "            W_k = self.W_ks[index].to(device)\n",
    "            W_v = self.W_vs[index].to(device)\n",
    "\n",
    "            q = W_q(encodings.to(device))\n",
    "            k = W_k(encodings.to(device))\n",
    "            v = W_v(encodings.to(device))\n",
    "\n",
    "            k_transposed = k.transpose(-1, -2)\n",
    "            sims = torch.matmul(q, k_transposed)\n",
    "            scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "                scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "            attention_percents = F.softmax(scaled_sims)\n",
    "            attention_scores = torch.matmul(attention_percents, v)\n",
    "            attentionscores.append(attention_scores)\n",
    "\n",
    "        combined_attention_scores = torch.cat(attentionscores, dim=-1)\n",
    "        combined_output = self.unify_heads(combined_attention_scores)\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c112a0b-ceed-43ff-b35e-89e3744b2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_tokens, using_mask=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, heads=num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.fc_layer2 = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        dropout=0.1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, position_encoded, mask=None):\n",
    "        if self.using_mask:\n",
    "            self_attention_values = self.self_attention(position_encoded, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(position_encoded)\n",
    "\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        normalized_values1 = self.layer_norm1(residual_connection_values)\n",
    "\n",
    "        fc_layer_output_relu = self.relu(self.fc_layer(normalized_values1))\n",
    "        #fc_layer_output_dropout = self.dropout(fc_layer_output_relu)\n",
    "        #fc_layer_output = self.fc_layer2(fc_layer_output_dropout)\n",
    "        #final_output = self.layer_norm2(normalized_values1 + fc_layer_output)\n",
    "        #fc_layer_output = self.fc_layer2(self.dropout(self.relu(self.fc_layer(normalized_values1))))\n",
    "        #return final_output\n",
    "        #fc_layer_output = self.fc_layer(normalized_values1)\n",
    "        fc_layer_output = self.relu(self.fc_layer(normalized_values1))\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200a8705-c790-4346-9182-b74c443d1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformerBlockTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super(DecoderOnlyTransformerBlockTransformer, self).__init__()\n",
    "        self.number_heads = 12\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.decoder_block1 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        self.decoder_block2 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        self.decoder_block3 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block4 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block5 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block6 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        if self.decoder_block1.using_mask:\n",
    "            mask_ones = torch.ones((token_ids.size(dim=1), token_ids.size(dim=1)))\n",
    "            mask = torch.tril(mask_ones)\n",
    "            mask = mask == 0\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        output_block1 = self.decoder_block1(position_encoded, mask=mask)\n",
    "        output_block2 = self.decoder_block2(output_block1, mask=mask)\n",
    "        output_block3 = self.decoder_block3(output_block2, mask=mask)\n",
    "        #output_block4 = self.decoder_block4(output_block3, mask=mask)\n",
    "        #output_block5 = self.decoder_block5(output_block4, mask=mask)\n",
    "        #output_block6 = self.decoder_block6(output_block5, mask=mask)\n",
    "\n",
    "        fc_layer_output = self.fc_layer(output_block3)\n",
    "\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bc46f5-5711-479d-8498-56f484965e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super().__init__()\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, heads=8)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # calculate word embeddings out of the tokens\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        # apply positional encoding (using the PositionalEncoder layer) to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        # create mask for decoder only transformer so it can not cheat\n",
    "        if (self.using_mask == True):\n",
    "            mask_ones = torch.ones((token_ids.size(dim=1), token_ids.size(dim=1)))\n",
    "            mask = torch.tril(mask_ones)\n",
    "            mask = mask == 0\n",
    "            # calculate self attention with the Attention Layer\n",
    "            self_attention_values = self.self_attention(position_encoded, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(position_encoded)\n",
    "        # add original position_encoded values to the calculated self attention values (residual connection)\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        normalized_values1 = self.layer_norm1(residual_connection_values)\n",
    "        fc_layer_output = self.fc_layer(normalized_values1)\n",
    "\n",
    "        # use the final linear layer to calculate the output probabilities\n",
    "        #fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cace8d-2d99-46db-875d-5733f92d2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"upstage/Pretraining_Dataset\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "987d9570-c84f-4c51-a96d-6dc172a30e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length: 100\n",
      "created vocab size: 7040\n",
      "the: 0\n",
      "project: 1\n",
      "gutenberg: 2\n",
      "ebook: 3\n",
      "of: 4\n",
      "alice's: 5\n",
      "adventures: 6\n",
      "in: 7\n",
      "wonderland\n",
      ": 8\n",
      ": 9\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "7040\n",
      "7040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class Text8Dataset(Dataset):\n",
    "    def __init__(self, file_path, sequence_length):\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            self.text = f.read()\n",
    "        self.min_length = 100\n",
    "        self.text = self.remove_punctuation(self.text)\n",
    "        self.sequence_length = sequence_length\n",
    "        print(f'sequence length: {self.sequence_length}')\n",
    "        self.vocab = self.create_vocabulary(file_path)\n",
    "        print(f'created vocab size: {len(self.vocab)}')\n",
    "        for i, (word, count) in enumerate(self.vocab.items()):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(f'{word}: {count}')\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        words = self.text.split()\n",
    "        self.text_as_int = [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        # Definiere die zu entfernenden Zeichen\n",
    "        punctuation = [',', '\"', \"'\", '.', ';', ':', '!', '?', '_', '“', '‘', '(', ')']\n",
    "\n",
    "        # Ersetze jedes Zeichen in punctuation durch einen leeren String\n",
    "        for char in punctuation:\n",
    "            text = text.replace(char, '')\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_vocabulary(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "        vocab = defaultdict(int)\n",
    "        index = 0\n",
    "\n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "\n",
    "        # Add special tokens\n",
    "        vocab['<EOS>'] = index\n",
    "        index += 1\n",
    "        vocab['<PAD>'] = index\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_int) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.text_as_int[idx:idx+self.sequence_length]\n",
    "        target_seq = self.text_as_int[idx+1:idx+self.sequence_length+1]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# Beispiel wie man das Dataset nutzt\n",
    "sequence_length = 100  # Länge der Sequenz\n",
    "#file_path = './datasets/text8.txt'  # Pfad zur Text8-Datei\n",
    "#file_path = './datasets/alice.txt'  # Pfad zur alice-Datei\n",
    "#file_path = \"/content/drive/MyDrive/datasets/alice.txt\"\n",
    "file_path = './datasets/treatise_of_human_nature.txt'\n",
    "\n",
    "dataset = Text8Dataset(file_path, sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Zugriff auf ein Batch\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(input_seq.shape)  # (batch_size, sequence_length)\n",
    "    print(target_seq.shape)  # (batch_size, sequence_length)\n",
    "    break\n",
    "\n",
    "token_to_id = dataset.vocab\n",
    "print(len(token_to_id))\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(id_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3654058-5df7-4264-a845-2312082409fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_37084\\1332413963.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 5.826795768115005\n",
      "Epoch 1, Train Loss: 4.922311894898297\n",
      "Epoch 2, Train Loss: 4.140790677517488\n",
      "Epoch 3, Train Loss: 3.5592738381173006\n",
      "Epoch 4, Train Loss: 3.0866622420103837\n",
      "Epoch 5, Train Loss: 2.726003172904943\n",
      "Epoch 6, Train Loss: 2.449965819159129\n",
      "Epoch 7, Train Loss: 2.2485720697992906\n",
      "Epoch 8, Train Loss: 2.0911445463192027\n",
      "Epoch 9, Train Loss: 1.9251614667897956\n",
      "Epoch 10, Train Loss: 1.7946303841034659\n",
      "Epoch 11, Train Loss: 1.6910913865303996\n",
      "Epoch 12, Train Loss: 1.590948512895691\n",
      "Epoch 13, Train Loss: 1.5041507127256493\n",
      "Epoch 14, Train Loss: 1.4301396057144922\n",
      "Epoch 15, Train Loss: 1.3600492415239303\n",
      "Epoch 16, Train Loss: 1.2862502792876203\n",
      "Epoch 17, Train Loss: 1.2364798078867794\n",
      "Epoch 18, Train Loss: 1.1698355076220208\n",
      "Epoch 19, Train Loss: 1.1246915960340198\n",
      "Epoch 20, Train Loss: 1.0786409505945704\n",
      "Epoch 21, Train Loss: 1.0353536207998852\n",
      "Epoch 22, Train Loss: 0.9960346187043662\n",
      "Epoch 23, Train Loss: 0.965520780092179\n",
      "Epoch 24, Train Loss: 0.9246659479405314\n",
      "Epoch 25, Train Loss: 0.8964214073763419\n",
      "Epoch 26, Train Loss: 0.859899388445656\n",
      "Epoch 27, Train Loss: 0.8301849697101471\n",
      "Epoch 28, Train Loss: 0.8053701991252787\n",
      "Epoch 29, Train Loss: 0.7771565016722742\n",
      "Epoch 30, Train Loss: 0.753047177182153\n",
      "Epoch 31, Train Loss: 0.7327247773728472\n",
      "Epoch 32, Train Loss: 0.7087851080232455\n",
      "Epoch 33, Train Loss: 0.6897654944972201\n",
      "Epoch 34, Train Loss: 0.6691574947280583\n",
      "Epoch 35, Train Loss: 0.6527221659881631\n",
      "Epoch 36, Train Loss: 0.633856121526805\n",
      "Epoch 37, Train Loss: 0.6207973255768208\n",
      "Epoch 38, Train Loss: 0.6049080605513367\n",
      "Epoch 39, Train Loss: 0.5884534859008214\n",
      "Epoch 40, Train Loss: 0.5729970685671976\n",
      "Epoch 41, Train Loss: 0.5606542056262589\n",
      "Epoch 42, Train Loss: 0.5478337534197252\n",
      "Epoch 43, Train Loss: 0.5354129646522238\n",
      "Epoch 44, Train Loss: 0.5254754188224343\n",
      "Epoch 45, Train Loss: 0.5143201245665904\n",
      "Epoch 46, Train Loss: 0.5010077618387918\n",
      "Epoch 47, Train Loss: 0.49272005493662535\n",
      "Epoch 48, Train Loss: 0.48376719836474535\n",
      "Epoch 49, Train Loss: 0.47259525509242967\n",
      "Epoch 50, Train Loss: 0.4631203915688456\n",
      "Epoch 51, Train Loss: 0.45219007599506456\n",
      "Epoch 52, Train Loss: 0.4449964456962231\n",
      "Epoch 53, Train Loss: 0.4355303495578937\n",
      "Epoch 54, Train Loss: 0.4294368477230025\n",
      "Epoch 55, Train Loss: 0.4201873877042224\n",
      "Epoch 56, Train Loss: 0.4161412926557743\n",
      "Epoch 57, Train Loss: 0.4078686889240471\n",
      "Epoch 58, Train Loss: 0.3984517744859154\n",
      "Epoch 59, Train Loss: 0.3944366910223863\n",
      "Epoch 60, Train Loss: 0.38670210084956813\n",
      "Epoch 61, Train Loss: 0.3820698592167614\n",
      "Epoch 62, Train Loss: 0.37369890073896433\n",
      "Epoch 63, Train Loss: 0.3679146755819415\n",
      "Epoch 64, Train Loss: 0.365362704485167\n",
      "Epoch 65, Train Loss: 0.3551332957012014\n",
      "Epoch 66, Train Loss: 0.35252422074124706\n",
      "Epoch 67, Train Loss: 0.34469085522315124\n",
      "Epoch 68, Train Loss: 0.34029768998411997\n",
      "Epoch 69, Train Loss: 0.33461024510517706\n",
      "Epoch 70, Train Loss: 0.33152915365357666\n",
      "Epoch 71, Train Loss: 0.3270628452958188\n",
      "Epoch 72, Train Loss: 0.323393594856208\n",
      "Epoch 73, Train Loss: 0.31640319857936244\n",
      "Epoch 74, Train Loss: 0.31299846744923565\n",
      "Epoch 75, Train Loss: 0.3089746432610317\n",
      "Epoch 76, Train Loss: 0.30400015861010815\n",
      "Epoch 77, Train Loss: 0.3005325476128013\n",
      "Epoch 78, Train Loss: 0.2982148912690814\n",
      "Epoch 79, Train Loss: 0.29255291123991145\n",
      "Epoch 80, Train Loss: 0.2892875901873851\n",
      "Epoch 81, Train Loss: 0.28436320443339813\n",
      "Epoch 82, Train Loss: 0.2821042448567855\n",
      "Epoch 83, Train Loss: 0.27856564116118043\n",
      "Epoch 84, Train Loss: 0.27498405519185726\n",
      "Epoch 85, Train Loss: 0.27095714961134765\n",
      "Epoch 86, Train Loss: 0.26850224759032554\n",
      "Epoch 87, Train Loss: 0.26397460790073163\n",
      "Epoch 88, Train Loss: 0.2619448232975294\n",
      "Epoch 89, Train Loss: 0.2593951252906865\n",
      "Epoch 90, Train Loss: 0.257165591459563\n",
      "Epoch 91, Train Loss: 0.25354335975549913\n",
      "Epoch 92, Train Loss: 0.25029331131014204\n",
      "Epoch 93, Train Loss: 0.2487053024906328\n",
      "Epoch 94, Train Loss: 0.24446016961043446\n",
      "Epoch 95, Train Loss: 0.2406210413103279\n",
      "Epoch 96, Train Loss: 0.23986487576508783\n",
      "Epoch 97, Train Loss: 0.23695180072562808\n",
      "Epoch 98, Train Loss: 0.2353727276058721\n",
      "Epoch 99, Train Loss: 0.23277479696163603\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "max_len = 100\n",
    "token_to_id = dataset.vocab\n",
    "#print(token_to_id)\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(token_to_id))\n",
    "\n",
    "#dimension_model = 768\n",
    "dimension_model = 256\n",
    "\n",
    "transformer_model = DecoderOnlyTransformerBlockTransformer(num_tokens=len(token_to_id), d_model=dimension_model, max_len=max_len)\n",
    "transformer_model.to(device)\n",
    "optimizer = Adam(transformer_model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens, labels = data\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        # Debugging: Ausgabe der maximalen und minimalen Werte von input_seq\n",
    "        #print(f\"Input Seq - Max Index: {input_seq.max().item()}, Min Index: {input_seq.min().item()}\")\n",
    "        #print(input_tokens.shape)\n",
    "        prediction = transformer_model(input_tokens)\n",
    "        prediction = prediction.view(-1, prediction.size(-1))  # [batch_size * seq_length, num_tokens]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length\n",
    "        loss = criterion(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step(epoch_loss)\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afa6c034-23fc-4567-90f6-b358d79f6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_37084\\1332413963.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy - could -  [an, all, course, just, could, ] \n",
      "She sells seashells by the - about -  [well, all, was, very, about, ] \n",
      "How much wood would a woodchuck chuck if a woodchuck could chuck - but -  [herself, thought, is, think, but, ] \n",
      "To be or not to be, that is the - to -  [herself, thought, but, must, to, ] \n",
      "All that glitters is not - see -  [never, herself, got, eyes, see, ] \n",
      "A journey of a thousand miles begins with a single - any -  [don’t, me, these, few, any, ] \n",
      "Beauty is in the eye of the - and -  [sat, to, the, of, and, ] \n",
      "Actions speak louder than - nothing -  [that’s, thought, to, would, nothing, ] \n",
      "The early bird catches the - herself -  [nothing, one, the, any, herself, ] \n",
      "A picture is worth a thousandOnce upon a time, in a land far, far away, there lived a brave  - would -  [her, very, the, off, would, ] \n",
      "The stars in the night sky were bright and beautiful, lighting up the  - much -  [herself, talking, it, was, much, ] \n",
      "In the middle of the forest, there was a small, hidden cottage made of  - must -  [well, upon, of, on, must, ] \n",
      "He who laughs last laughs  - he -  [all, well, even, and, he, ] \n",
      "Every cloud has a silver  - with -  [could, eyes, hurried, he, with, ] \n",
      "It's always darkest before the  - at -  [the, which, to, because, at, ] \n",
      "When the going gets tough, the tough get - just -  [out, all, eyes, under, just, ] \n",
      "Two heads are better than  - on -  [herself, up, one, down, on, ] \n",
      "A watched pot never  - how -  [course, it, could, she, how, ] \n",
      "Honesty is the best  - it -  [don’t, was, for, all, it, ] \n",
      "Alice was not a bit hurt, and she - could -  [herself, thought, it, beg, could, ] \n",
      "Alice opened the door and found that - get -  [herself, thought, work, is, get, ] \n",
      "After a while, finding that nothing more happened - thought -  [herself, their, out, nothing, thought, ] \n",
      "Just then her head struck - out -  [thought, her, ever, to, out, ] \n",
      "As she said this she looked down at her - all -  [herself, down, must, well, all, ] \n",
      "won’t talk about cats or - must -  [herself, out, many, very, must, ] \n",
      "easy to - herself -  [well, could, up, what, herself, ] \n"
     ]
    }
   ],
   "source": [
    "# Testtexte\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy\", #dog\n",
    "    \"She sells seashells by the\", #seashore\n",
    "    \"How much wood would a woodchuck chuck if a woodchuck could chuck\", #wood\n",
    "    \"To be or not to be, that is the\", #question.\n",
    "    \"All that glitters is not\", #gold\n",
    "    \"A journey of a thousand miles begins with a single\", #step\n",
    "    \"Beauty is in the eye of the\", #beholder\n",
    "    \"Actions speak louder than\", #words\n",
    "    \"The early bird catches the\", #worm\n",
    "    \"A picture is worth a thousand\" #words,\n",
    "    \"Once upon a time, in a land far, far away, there lived a brave \", #knight.\n",
    "    \"The stars in the night sky were bright and beautiful, lighting up the \", #darkness.\n",
    "    \"In the middle of the forest, there was a small, hidden cottage made of \", #gingerbread.\n",
    "    \"He who laughs last laughs \", #longest.\n",
    "    \"Every cloud has a silver \", #lining.\n",
    "    \"It's always darkest before the \", #dawn.\n",
    "    \"When the going gets tough, the tough get\", #going.\n",
    "    \"Two heads are better than \", #one.\n",
    "    \"A watched pot never \", #boils.\n",
    "    \"Honesty is the best \", #policy.\n",
    "    \"Alice was not a bit hurt, and she\",\n",
    "    \"Alice opened the door and found that\",\n",
    "    \"After a while, finding that nothing more happened\",\n",
    "    \"Just then her head struck\",\n",
    "    \"As she said this she looked down at her\", #hands\n",
    "    \"won’t talk about cats or\", #hands\n",
    "    \"easy to\"\n",
    "]\n",
    "\n",
    "def string_to_model_input(input_string):\n",
    "    # Split the input string into tokens\n",
    "    tokens = input_string.lower().split()\n",
    "\n",
    "    model_input = []\n",
    "    for token in tokens:\n",
    "        if token in token_to_id:\n",
    "            model_input.append(token_to_id[token])\n",
    "\n",
    "    model_input.append(token_to_id['<EOS>'])\n",
    "    model_input_tensor = torch.tensor(model_input)\n",
    "    return model_input_tensor\n",
    "\n",
    "# Schleife zum Testen des Transformers\n",
    "for text in test_texts:\n",
    "    model_input_expanded = string_to_model_input(text)\n",
    "    model_input_expanded = model_input_expanded.to(device)\n",
    "    model_input_expanded = model_input_expanded.unsqueeze(0)\n",
    "    input_length = model_input_expanded.size(dim=0)\n",
    "    predictions = transformer_model(model_input_expanded)\n",
    "\n",
    "    last_predictions = predictions[-1, :]\n",
    "\n",
    "    max_index = torch.argmax(last_predictions[-1,:])\n",
    "\n",
    "    predicted_id = torch.tensor([max_index], device=device)\n",
    "    predicted_ids = predicted_id\n",
    "\n",
    "    for id in predicted_ids:\n",
    "        topk_values, topk_indices = torch.topk(last_predictions[-1,:], k=5)\n",
    "\n",
    "        # Convert top indices to tokens\n",
    "        predicted_ids = topk_indices\n",
    "        possible_tokens = \" [\"\n",
    "        for id in predicted_ids:\n",
    "            possible_tokens += id_to_token[id.item()] + \", \"\n",
    "        possible_tokens += \"] \"\n",
    "\n",
    "        #print(f\"{id_to_token[id.item()]}\")\n",
    "        print(f\"{text} - {id_to_token[id.item()]} - {possible_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbba02-07a7-4c5c-ac52-a58e4781448e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefaf95d-44e0-41fa-b59f-616baab75ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab3224-ea73-40ed-8736-17d0e13777c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3578c-f665-4d24-89c4-efcb8b6aacb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb818c12-3267-42d1-8ef8-59be79ac9799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a27ae-5a32-46f2-af32-e1d1732d4421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865a1a2-39cc-40ba-81d1-89e54338a82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
