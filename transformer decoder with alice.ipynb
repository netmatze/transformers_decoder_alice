{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d438488f-b9ca-432c-8e6a-b7efb1ce50fc",
   "metadata": {},
   "source": [
    "## Transformers decoder only (gpt2 like) trained at Lewis Carrolls Alice's Adventures in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21e1e00-f3d9-4d5c-b112-76c6cde83584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./content/drive/MyDrive/datasets/alice.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#import lightning as L\n",
    "\n",
    "\"./content/drive/MyDrive/datasets/alice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9c1ec1-0f76-417f-83cc-2ad0c6728cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4a3eaab-b15f-4d2b-a275-3a883cb07817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        div_term = 1 / torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        #print(div_term)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        pe_temp = self.pe[:word_embeddings.size(0), :]\n",
    "        pe_temp_expanded = pe_temp.unsqueeze(1)\n",
    "        #print(f\"word_embeddings.shape: {word_embeddings.shape}, self.pe.shape: {pe_temp_expanded.shape}, \")\n",
    "        return word_embeddings + pe_temp_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67ef9c2c-f4b7-4bcc-a410-f4b74e67e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        q = self.W_q(encodings)\n",
    "        k = self.W_k(encodings)\n",
    "        v = self.W_v(encodings)\n",
    "\n",
    "        # Ensure k has the same shape as q before transpose\n",
    "        assert k.shape == q.shape\n",
    "\n",
    "        # Transpose k to align with q for dot product\n",
    "        k_transposed = k.transpose(-1, -2)\n",
    "\n",
    "        # Check shapes\n",
    "        #print(\"Shape of q:\", q.shape)  # [1, 5, 2]\n",
    "        #print(\"Shape of k_transposed:\", k_transposed.shape)  # [1, 2, 5]\n",
    "        sims = torch.matmul(q, k_transposed)\n",
    "        scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23aac6e2-bfce-4ed4-91f0-b344ce83399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=2,heads=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_qs = []\n",
    "        self.W_ks = []\n",
    "        self.W_vs = []\n",
    "\n",
    "        for index in range(heads):\n",
    "            W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "            W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "            self.W_qs.append(W_q)\n",
    "            self.W_ks.append(W_k)\n",
    "            self.W_vs.append(W_v)\n",
    "\n",
    "        self.unify_heads = nn.Linear(d_model * heads, d_model)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self, encodings, mask=None):\n",
    "        attentionscores = []\n",
    "        #encodings.to(device)\n",
    "        for index in range(self.heads):\n",
    "            W_q = self.W_qs[index].to(device)\n",
    "            W_k = self.W_ks[index].to(device)\n",
    "            W_v = self.W_vs[index].to(device)\n",
    "\n",
    "            q = W_q(encodings.to(device))\n",
    "            k = W_k(encodings.to(device))\n",
    "            v = W_v(encodings.to(device))\n",
    "\n",
    "            k_transposed = k.transpose(-1, -2)\n",
    "            sims = torch.matmul(q, k_transposed)\n",
    "            scaled_sims = sims / torch.tensor(k.size(1)**0.5)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.to(device)\n",
    "                scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "            attention_percents = F.softmax(scaled_sims)\n",
    "            attention_scores = torch.matmul(attention_percents, v)\n",
    "            attentionscores.append(attention_scores)\n",
    "\n",
    "        combined_attention_scores = torch.cat(attentionscores, dim=-1)\n",
    "        combined_output = self.unify_heads(combined_attention_scores)\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c112a0b-ceed-43ff-b35e-89e3744b2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_tokens, using_mask=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, heads=num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.fc_layer2 = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        dropout=0.1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, position_encoded, mask=None):\n",
    "        if self.using_mask:\n",
    "            self_attention_values = self.self_attention(position_encoded, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(position_encoded)\n",
    "\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        normalized_values1 = self.layer_norm1(residual_connection_values)\n",
    "\n",
    "        fc_layer_output_relu = self.relu(self.fc_layer(normalized_values1))\n",
    "        #fc_layer_output_dropout = self.dropout(fc_layer_output_relu)\n",
    "        #fc_layer_output = self.fc_layer2(fc_layer_output_dropout)\n",
    "        #final_output = self.layer_norm2(normalized_values1 + fc_layer_output)\n",
    "        #fc_layer_output = self.fc_layer2(self.dropout(self.relu(self.fc_layer(normalized_values1))))\n",
    "        #return final_output\n",
    "        #fc_layer_output = self.fc_layer(normalized_values1)\n",
    "        fc_layer_output = self.relu(self.fc_layer(normalized_values1))\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "200a8705-c790-4346-9182-b74c443d1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformerBlockTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super(DecoderOnlyTransformerBlockTransformer, self).__init__()\n",
    "        self.number_heads = 2\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.decoder_block1 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block2 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block3 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block4 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block5 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "        #self.decoder_block6 = DecoderBlock(d_model=d_model, num_heads=self.number_heads, num_tokens=num_tokens, using_mask=using_mask)\n",
    "\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        if self.decoder_block1.using_mask:\n",
    "            mask_ones = torch.ones((token_ids.size(dim=1), token_ids.size(dim=1)))\n",
    "            mask = torch.tril(mask_ones)\n",
    "            mask = mask == 0\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        output_block1 = self.decoder_block1(position_encoded, mask=mask)\n",
    "        #output_block2 = self.decoder_block2(output_block1, mask=mask)\n",
    "        #output_block3 = self.decoder_block3(output_block2, mask=mask)\n",
    "        #output_block4 = self.decoder_block4(output_block3, mask=mask)\n",
    "        #output_block5 = self.decoder_block5(output_block4, mask=mask)\n",
    "        #output_block6 = self.decoder_block6(output_block5, mask=mask)\n",
    "\n",
    "        fc_layer_output = self.fc_layer(output_block1)\n",
    "\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5bc46f5-5711-479d-8498-56f484965e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len, using_mask=True):\n",
    "        super().__init__()\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, heads=8)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.using_mask = using_mask\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # calculate word embeddings out of the tokens\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        # apply positional encoding (using the PositionalEncoder layer) to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "        # create mask for decoder only transformer so it can not cheat\n",
    "        if (self.using_mask == True):\n",
    "            mask_ones = torch.ones((token_ids.size(dim=1), token_ids.size(dim=1)))\n",
    "            mask = torch.tril(mask_ones)\n",
    "            mask = mask == 0\n",
    "            # calculate self attention with the Attention Layer\n",
    "            self_attention_values = self.self_attention(position_encoded, mask=mask)\n",
    "        else:\n",
    "            self_attention_values = self.self_attention(position_encoded)\n",
    "        # add original position_encoded values to the calculated self attention values (residual connection)\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        normalized_values1 = self.layer_norm1(residual_connection_values)\n",
    "        fc_layer_output = self.fc_layer(normalized_values1)\n",
    "\n",
    "        # use the final linear layer to calculate the output probabilities\n",
    "        #fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "\n",
    "        return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13cace8d-2d99-46db-875d-5733f92d2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"upstage/Pretraining_Dataset\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef02d6-e4d3-404e-b3ec-ac9ba6446b32",
   "metadata": {},
   "source": [
    "## create custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3cc3faa-ec52-4bd2-8db4-b2306b1a6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {'what' : 0 ,'is' : 1 ,'statquest' : 2 ,'awesome' : 3 ,'<EOS>' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e0e3013-058b-4125-a58d-65fd42aa3f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 4, 3],\n",
       "        [2, 1, 0, 4, 3]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {'what' : 0 ,'is' : 1 ,'statquest' : 2 ,'awesome' : 3 ,'<EOS>' : 4}\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "inputs = torch.tensor([[token_to_id['what'],token_to_id['is'], token_to_id['statquest'], \n",
    "                        token_to_id['<EOS>'], token_to_id['awesome']],\n",
    "                       [token_to_id['statquest'],token_to_id['is'], token_to_id['what'], \n",
    "                        token_to_id['<EOS>'], token_to_id['awesome']]])\n",
    "labels = torch.tensor([[token_to_id['is'],token_to_id['statquest'], token_to_id['<EOS>'], \n",
    "                        token_to_id['awesome'], token_to_id['<EOS>']],\n",
    "                       [token_to_id['is'],token_to_id['what'], token_to_id['<EOS>'], \n",
    "                        token_to_id['awesome'], token_to_id['<EOS>']]])\n",
    "custom_dataset = TensorDataset(inputs, labels)\n",
    "custom_dataloader = DataLoader(custom_dataset)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89e04e04-cbeb-4a0b-8050-fa9b15893fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length: 3\n",
      "created vocab size: 4\n",
      "what: 0\n",
      "is: 1\n",
      "statquest: 2\n",
      "awesome: 3\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, texts, sequence_length):\n",
    "        self.min_length = 100\n",
    "        self.texts = texts\n",
    "        self.texts = self.remove_punctuation(self.texts)\n",
    "        self.sequence_length = sequence_length\n",
    "        print(f'sequence length: {self.sequence_length}')\n",
    "        self.vocab = self.create_vocabulary(self.texts)\n",
    "        print(f'created vocab size: {len(self.vocab)}')\n",
    "        for i, (word, count) in enumerate(self.vocab.items()):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(f'{word}: {count}')\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        words = self.texts.split()\n",
    "        self.text_as_int = [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        # Definiere die zu entfernenden Zeichen\n",
    "        punctuation = [',', '\"', \"'\", '.', ';', ':', '!', '?', '_', '“', '‘', '(', ')']\n",
    "\n",
    "        # Ersetze jedes Zeichen in punctuation durch einen leeren String\n",
    "        for char in punctuation:\n",
    "            text = text.replace(char, '')\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_vocabulary(self, texts):\n",
    "        vocab = defaultdict(int)\n",
    "        index = 0\n",
    "\n",
    "        words = texts.split(' ')\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "\n",
    "        # Add special tokens\n",
    "        #vocab['<EOS>'] = index\n",
    "        #index += 1\n",
    "        #vocab['<PAD>'] = index\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_int) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.text_as_int[idx:idx+self.sequence_length]\n",
    "        target_seq = self.text_as_int[idx+1:idx+self.sequence_length+1]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "sequence_length = 3  # Länge der Sequenz\n",
    "\n",
    "texts = \"what is statquest awesome. statquest is what awesome. is what awesome?\"\n",
    "\n",
    "simple_dataset = SimpleDataset(texts, sequence_length)\n",
    "simple_dataloader = DataLoader(simple_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Zugriff auf ein Batch\n",
    "for input_seq, target_seq in simple_dataloader:\n",
    "    print(input_seq.shape)  # (batch_size, sequence_length)\n",
    "    print(target_seq.shape)  # (batch_size, sequence_length)\n",
    "    break\n",
    "\n",
    "token_to_id = simple_dataset.vocab\n",
    "print(len(token_to_id))\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(id_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fbddfcd-8b48-45af-a1b7-6fe5a35197b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Epoch 0, Train Loss: 6.286625146865845\n",
      "Epoch 1, Train Loss: 5.0406370759010315\n",
      "Epoch 2, Train Loss: 4.115444898605347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_18956\\1332413963.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 3.51344233751297\n",
      "Epoch 4, Train Loss: 2.9851282238960266\n",
      "Epoch 5, Train Loss: 2.566750556230545\n",
      "Epoch 6, Train Loss: 2.2349943220615387\n",
      "Epoch 7, Train Loss: 1.6051430404186249\n",
      "Epoch 8, Train Loss: 1.6375124230980873\n",
      "Epoch 9, Train Loss: 1.618441864848137\n",
      "Epoch 10, Train Loss: 1.3670375645160675\n",
      "Epoch 11, Train Loss: 1.1052008382976055\n",
      "Epoch 12, Train Loss: 1.1439470648765564\n",
      "Epoch 13, Train Loss: 1.2480268776416779\n",
      "Epoch 14, Train Loss: 1.1249349117279053\n",
      "Epoch 15, Train Loss: 1.1307107470929623\n",
      "Epoch 16, Train Loss: 1.4947677850723267\n",
      "Epoch 17, Train Loss: 0.9342371970415115\n",
      "Epoch 18, Train Loss: 0.9124922379851341\n",
      "Epoch 19, Train Loss: 1.029920257627964\n",
      "Epoch 20, Train Loss: 0.7956928312778473\n",
      "Epoch 21, Train Loss: 0.8087613843381405\n",
      "Epoch 22, Train Loss: 1.0170997306704521\n",
      "Epoch 23, Train Loss: 0.7713101878762245\n",
      "Epoch 24, Train Loss: 0.8222260558977723\n",
      "Epoch 25, Train Loss: 0.8104960024356842\n",
      "Epoch 26, Train Loss: 0.8200611933134496\n",
      "Epoch 27, Train Loss: 0.6570381037890911\n",
      "Epoch 28, Train Loss: 0.6150289513170719\n",
      "Epoch 29, Train Loss: 0.4890771098434925\n",
      "Epoch 30, Train Loss: 0.531559987924993\n",
      "Epoch 31, Train Loss: 1.0161163173615932\n",
      "Epoch 32, Train Loss: 0.7131935823708773\n",
      "Epoch 33, Train Loss: 1.2330859024077654\n",
      "Epoch 34, Train Loss: 0.4147318098694086\n",
      "Epoch 35, Train Loss: 0.5484681595116854\n",
      "Epoch 36, Train Loss: 0.41010243771597743\n",
      "Epoch 37, Train Loss: 0.5360126569867134\n",
      "Epoch 38, Train Loss: 0.37962305429391563\n",
      "Epoch 39, Train Loss: 0.4625752530992031\n",
      "Epoch 40, Train Loss: 0.350961122661829\n",
      "Epoch 41, Train Loss: 0.5022878260351717\n",
      "Epoch 42, Train Loss: 0.8082046955823898\n",
      "Epoch 43, Train Loss: 2.1456637971568853\n",
      "Epoch 44, Train Loss: 0.7541657765395939\n",
      "Epoch 45, Train Loss: 0.6633920967578888\n",
      "Epoch 46, Train Loss: 0.7821187227964401\n",
      "Epoch 47, Train Loss: 0.45634420588612556\n",
      "Epoch 48, Train Loss: 0.3109030909836292\n",
      "Epoch 49, Train Loss: 0.2814635597169399\n",
      "Epoch 50, Train Loss: 0.32527193194255233\n",
      "Epoch 51, Train Loss: 0.5000701959943399\n",
      "Epoch 52, Train Loss: 0.2554389634169638\n",
      "Epoch 53, Train Loss: 0.48815890014520846\n",
      "Epoch 54, Train Loss: 0.22788823768496513\n",
      "Epoch 55, Train Loss: 0.11019291076809168\n",
      "Epoch 56, Train Loss: 0.9997075167484581\n",
      "Epoch 57, Train Loss: 0.09164931392297149\n",
      "Epoch 58, Train Loss: 0.21824875952734146\n",
      "Epoch 59, Train Loss: 0.21828651713440195\n",
      "Epoch 60, Train Loss: 0.4365379945375025\n",
      "Epoch 61, Train Loss: 0.1857522097416222\n",
      "Epoch 62, Train Loss: 0.16146484547061846\n",
      "Epoch 63, Train Loss: 0.07512907224008814\n",
      "Epoch 64, Train Loss: 0.4944446124136448\n",
      "Epoch 65, Train Loss: 0.14229957363568246\n",
      "Epoch 66, Train Loss: 0.07636432885192335\n",
      "Epoch 67, Train Loss: 0.08923753397539258\n",
      "Epoch 68, Train Loss: 0.07888088026084006\n",
      "Epoch 69, Train Loss: 0.07450808327121194\n",
      "Epoch 70, Train Loss: 0.8421181398443878\n",
      "Epoch 71, Train Loss: 0.2907409034669399\n",
      "Epoch 72, Train Loss: 0.557528292760253\n",
      "Epoch 73, Train Loss: 0.17284437436319422\n",
      "Epoch 74, Train Loss: 0.46471824683248997\n",
      "Epoch 75, Train Loss: 0.11036403907928616\n",
      "Epoch 76, Train Loss: 0.2811315681610722\n",
      "Epoch 77, Train Loss: 0.1330998841440305\n",
      "Epoch 78, Train Loss: 0.36966625093191396\n",
      "Epoch 79, Train Loss: 0.32867419230751693\n",
      "Epoch 80, Train Loss: 0.1264583943411708\n",
      "Epoch 81, Train Loss: 0.10587395331822336\n",
      "Epoch 82, Train Loss: 0.28607415594160557\n",
      "Epoch 83, Train Loss: 0.09901443572016433\n",
      "Epoch 84, Train Loss: 0.09241561649832875\n",
      "Epoch 85, Train Loss: 0.0629481376381591\n",
      "Epoch 86, Train Loss: 0.10535824706312269\n",
      "Epoch 87, Train Loss: 0.09835217974614352\n",
      "Epoch 88, Train Loss: 0.08484325790777802\n",
      "Epoch 89, Train Loss: 0.14416175929363817\n",
      "Epoch 90, Train Loss: 0.052369868208188564\n",
      "Epoch 91, Train Loss: 0.09327122825197875\n",
      "Epoch 92, Train Loss: 0.05626205028966069\n",
      "Epoch 93, Train Loss: 0.09963941783644259\n",
      "Epoch 94, Train Loss: 0.043141187736182474\n",
      "Epoch 95, Train Loss: 0.12832212663488463\n",
      "Epoch 96, Train Loss: 0.08470679633319378\n",
      "Epoch 97, Train Loss: 0.477036114432849\n",
      "Epoch 98, Train Loss: 0.06915454153204337\n",
      "Epoch 99, Train Loss: 0.32538716588169336\n",
      "Epoch 100, Train Loss: 0.14437315752729774\n",
      "Epoch 101, Train Loss: 0.20743644252070226\n",
      "Epoch 102, Train Loss: 0.07797518608276732\n",
      "Epoch 103, Train Loss: 0.21815656078979373\n",
      "Epoch 104, Train Loss: 0.242550911847502\n",
      "Epoch 105, Train Loss: 0.2366673364304006\n",
      "Epoch 106, Train Loss: 0.23652760381810367\n",
      "Epoch 107, Train Loss: 0.291468120762147\n",
      "Epoch 108, Train Loss: 0.20083808909112122\n",
      "Epoch 109, Train Loss: 0.13028168119490147\n",
      "Epoch 110, Train Loss: 0.12080484163016081\n",
      "Epoch 111, Train Loss: 0.14295974955894053\n",
      "Epoch 112, Train Loss: 0.0943825920112431\n",
      "Epoch 113, Train Loss: 0.1077265619096579\n",
      "Epoch 114, Train Loss: 0.09194546297658235\n",
      "Epoch 115, Train Loss: 0.08378632343374193\n",
      "Epoch 116, Train Loss: 0.11133302433881909\n",
      "Epoch 117, Train Loss: 0.24004288320429623\n",
      "Epoch 118, Train Loss: 0.1277760376688093\n",
      "Epoch 119, Train Loss: 0.07425339263863862\n",
      "Epoch 120, Train Loss: 0.28817669732961804\n",
      "Epoch 121, Train Loss: 0.20429440180305392\n",
      "Epoch 122, Train Loss: 0.22350869898218662\n",
      "Epoch 123, Train Loss: 0.10671673063188791\n",
      "Epoch 124, Train Loss: 0.11120823118835688\n",
      "Epoch 125, Train Loss: 0.20009905111510307\n",
      "Epoch 126, Train Loss: 0.16141420416533947\n",
      "Epoch 127, Train Loss: 0.06608266942203045\n",
      "Epoch 128, Train Loss: 0.07795157528016716\n",
      "Epoch 129, Train Loss: 0.08710225163667928\n",
      "Epoch 130, Train Loss: 0.2860555666266009\n",
      "Epoch 131, Train Loss: 0.09310599234595429\n",
      "Epoch 132, Train Loss: 0.12589069572277367\n",
      "Epoch 133, Train Loss: 0.07502852566540241\n",
      "Epoch 134, Train Loss: 0.06432559003587812\n",
      "Epoch 135, Train Loss: 0.11189315654337406\n",
      "Epoch 136, Train Loss: 0.11354517773725092\n",
      "Epoch 137, Train Loss: 0.07349765370599926\n",
      "Epoch 138, Train Loss: 0.23459186108084396\n",
      "Epoch 139, Train Loss: 0.10676193609833717\n",
      "Epoch 140, Train Loss: 0.21391311986371875\n",
      "Epoch 141, Train Loss: 0.11456623068079352\n",
      "Epoch 142, Train Loss: 0.07836811314336956\n",
      "Epoch 143, Train Loss: 0.09181529271882027\n",
      "Epoch 144, Train Loss: 0.11285221931757405\n",
      "Epoch 145, Train Loss: 0.09792958572506905\n",
      "Epoch 146, Train Loss: 0.08925179461948574\n",
      "Epoch 147, Train Loss: 0.08543425844982266\n",
      "Epoch 148, Train Loss: 0.07174785400275141\n",
      "Epoch 149, Train Loss: 0.09466173748660367\n",
      "Epoch 150, Train Loss: 0.08076308201998472\n",
      "Epoch 151, Train Loss: 0.2885815354529768\n",
      "Epoch 152, Train Loss: 0.20291046076454222\n",
      "Epoch 153, Train Loss: 0.1304848229046911\n",
      "Epoch 154, Train Loss: 0.08390566683374345\n",
      "Epoch 155, Train Loss: 0.06909306778106838\n",
      "Epoch 156, Train Loss: 0.13597674551419914\n",
      "Epoch 157, Train Loss: 0.21465014421846718\n",
      "Epoch 158, Train Loss: 0.20623599219834432\n",
      "Epoch 159, Train Loss: 0.0879835122032091\n",
      "Epoch 160, Train Loss: 0.12340461771236733\n",
      "Epoch 161, Train Loss: 0.2334097558632493\n",
      "Epoch 162, Train Loss: 0.08826718223281205\n",
      "Epoch 163, Train Loss: 0.3167904410511255\n",
      "Epoch 164, Train Loss: 0.12735752353910357\n",
      "Epoch 165, Train Loss: 0.0703959388192743\n",
      "Epoch 166, Train Loss: 0.09958931209985167\n",
      "Epoch 167, Train Loss: 0.0943234299775213\n",
      "Epoch 168, Train Loss: 0.26378755271434784\n",
      "Epoch 169, Train Loss: 0.11136385682038963\n",
      "Epoch 170, Train Loss: 0.08502692193724215\n",
      "Epoch 171, Train Loss: 0.13798449328169227\n",
      "Epoch 172, Train Loss: 0.2026521093212068\n",
      "Epoch 173, Train Loss: 0.10944544756785035\n",
      "Epoch 174, Train Loss: 0.06812482851091772\n",
      "Epoch 175, Train Loss: 0.07052669074619189\n",
      "Epoch 176, Train Loss: 0.08527768810745329\n",
      "Epoch 177, Train Loss: 0.08384458906948566\n",
      "Epoch 178, Train Loss: 0.061596334213390946\n",
      "Epoch 179, Train Loss: 0.20257043454330415\n",
      "Epoch 180, Train Loss: 0.08074586553266272\n",
      "Epoch 181, Train Loss: 0.07094494020566344\n",
      "Epoch 182, Train Loss: 0.08104883383202832\n",
      "Epoch 183, Train Loss: 0.09828018461121246\n",
      "Epoch 184, Train Loss: 0.09437803365290165\n",
      "Epoch 185, Train Loss: 0.08743289136327803\n",
      "Epoch 186, Train Loss: 0.07717586006037891\n",
      "Epoch 187, Train Loss: 0.20140786096453667\n",
      "Epoch 188, Train Loss: 0.08971382328309119\n",
      "Epoch 189, Train Loss: 0.21676060324534774\n",
      "Epoch 190, Train Loss: 0.12295155040919781\n",
      "Epoch 191, Train Loss: 0.09904008194280323\n",
      "Epoch 192, Train Loss: 0.07064241380430758\n",
      "Epoch 193, Train Loss: 0.06844373210333288\n",
      "Epoch 194, Train Loss: 0.07085988180187996\n",
      "Epoch 195, Train Loss: 0.08454486494883895\n",
      "Epoch 196, Train Loss: 0.07823177962563932\n",
      "Epoch 197, Train Loss: 0.1441731583327055\n",
      "Epoch 198, Train Loss: 0.08805379644036293\n",
      "Epoch 199, Train Loss: 0.09266591654159129\n",
      "Epoch 200, Train Loss: 0.08795659038878512\n",
      "Epoch 201, Train Loss: 0.09295192791614681\n",
      "Epoch 202, Train Loss: 0.13043068883416709\n",
      "Epoch 203, Train Loss: 0.22153186208743136\n",
      "Epoch 204, Train Loss: 0.11000316007994115\n",
      "Epoch 205, Train Loss: 0.1562738325446844\n",
      "Epoch 206, Train Loss: 0.12289717979729176\n",
      "Epoch 207, Train Loss: 0.05197386466898024\n",
      "Epoch 208, Train Loss: 0.08885160437785089\n",
      "Epoch 209, Train Loss: 0.11111351165163796\n",
      "Epoch 210, Train Loss: 0.07032122719101608\n",
      "Epoch 211, Train Loss: 0.20140919170808047\n",
      "Epoch 212, Train Loss: 0.28670233010780066\n",
      "Epoch 213, Train Loss: 0.06172966840676963\n",
      "Epoch 214, Train Loss: 0.08779431832954288\n",
      "Epoch 215, Train Loss: 0.0612035698723048\n",
      "Epoch 216, Train Loss: 0.20967065345030278\n",
      "Epoch 217, Train Loss: 0.09317823220044374\n",
      "Epoch 218, Train Loss: 0.0676403904799372\n",
      "Epoch 219, Train Loss: 0.09455112367868423\n",
      "Epoch 220, Train Loss: 0.08884647069498897\n",
      "Epoch 221, Train Loss: 0.07887746638152748\n",
      "Epoch 222, Train Loss: 0.2215963572380133\n",
      "Epoch 223, Train Loss: 0.08157661859877408\n",
      "Epoch 224, Train Loss: 0.07814026587584522\n",
      "Epoch 225, Train Loss: 0.06914842245168984\n",
      "Epoch 226, Train Loss: 0.12150156556162983\n",
      "Epoch 227, Train Loss: 0.0880685371812433\n",
      "Epoch 228, Train Loss: 0.0866486157028703\n",
      "Epoch 229, Train Loss: 0.21694839652627707\n",
      "Epoch 230, Train Loss: 0.07813926540256944\n",
      "Epoch 231, Train Loss: 0.11238312692148611\n",
      "Epoch 232, Train Loss: 0.07643024472054094\n",
      "Epoch 233, Train Loss: 0.3003131541190669\n",
      "Epoch 234, Train Loss: 0.11004734225571156\n",
      "Epoch 235, Train Loss: 0.19762241648277268\n",
      "Epoch 236, Train Loss: 0.09827937063528225\n",
      "Epoch 237, Train Loss: 0.09308304684236646\n",
      "Epoch 238, Train Loss: 0.10036161832977086\n",
      "Epoch 239, Train Loss: 0.09361682925373316\n",
      "Epoch 240, Train Loss: 0.08080642342974897\n",
      "Epoch 241, Train Loss: 0.08770180433930364\n",
      "Epoch 242, Train Loss: 0.0716538491396932\n",
      "Epoch 243, Train Loss: 0.06545367695798632\n",
      "Epoch 244, Train Loss: 0.057847195537760854\n",
      "Epoch 245, Train Loss: 0.08830742887221277\n",
      "Epoch 246, Train Loss: 0.11564107856247574\n",
      "Epoch 247, Train Loss: 0.08234680665191263\n",
      "Epoch 248, Train Loss: 0.07047238793165889\n",
      "Epoch 249, Train Loss: 0.08023762270750012\n",
      "Epoch 250, Train Loss: 0.07995568959449884\n",
      "Epoch 251, Train Loss: 0.10923598567023873\n",
      "Epoch 252, Train Loss: 0.11915249330922961\n",
      "Epoch 253, Train Loss: 0.12255858606658876\n",
      "Epoch 254, Train Loss: 0.23947954986942932\n",
      "Epoch 255, Train Loss: 0.21220304106827825\n",
      "Epoch 256, Train Loss: 0.08847671250987332\n",
      "Epoch 257, Train Loss: 0.09792113653384149\n",
      "Epoch 258, Train Loss: 0.08126283681485802\n",
      "Epoch 259, Train Loss: 0.060485463254735805\n",
      "Epoch 260, Train Loss: 0.09203984844498336\n",
      "Epoch 261, Train Loss: 0.08474626741372049\n",
      "Epoch 262, Train Loss: 0.09909950791916344\n",
      "Epoch 263, Train Loss: 0.09618871542625129\n",
      "Epoch 264, Train Loss: 0.10390987864229828\n",
      "Epoch 265, Train Loss: 0.07051404043158982\n",
      "Epoch 266, Train Loss: 0.0695475315587828\n",
      "Epoch 267, Train Loss: 0.20337436138652265\n",
      "Epoch 268, Train Loss: 0.28821780171711\n",
      "Epoch 269, Train Loss: 0.09253477700985968\n",
      "Epoch 270, Train Loss: 0.13180211349390447\n",
      "Epoch 271, Train Loss: 0.12970225024037063\n",
      "Epoch 272, Train Loss: 0.10912524751620367\n",
      "Epoch 273, Train Loss: 0.09192154475022107\n",
      "Epoch 274, Train Loss: 0.12391378241591156\n",
      "Epoch 275, Train Loss: 0.28708301775623113\n",
      "Epoch 276, Train Loss: 0.08967865641170647\n",
      "Epoch 277, Train Loss: 0.08595150346809532\n",
      "Epoch 278, Train Loss: 0.05508597695734352\n",
      "Epoch 279, Train Loss: 0.1138260483276099\n",
      "Epoch 280, Train Loss: 0.10280456143664196\n",
      "Epoch 281, Train Loss: 0.28792947332840413\n",
      "Epoch 282, Train Loss: 0.07338879664894193\n",
      "Epoch 283, Train Loss: 0.12117112171836197\n",
      "Epoch 284, Train Loss: 0.07227659726049751\n",
      "Epoch 285, Train Loss: 0.0937119850423187\n",
      "Epoch 286, Train Loss: 0.2014927780837752\n",
      "Epoch 287, Train Loss: 0.08592184736335184\n",
      "Epoch 288, Train Loss: 0.07382698846049607\n",
      "Epoch 289, Train Loss: 0.2268758420686936\n",
      "Epoch 290, Train Loss: 0.0799535997066414\n",
      "Epoch 291, Train Loss: 0.07063778094016016\n",
      "Epoch 292, Train Loss: 0.28947341768071055\n",
      "Epoch 293, Train Loss: 0.21773850015597418\n",
      "Epoch 294, Train Loss: 0.28617209556978196\n",
      "Epoch 295, Train Loss: 0.09638101537711918\n",
      "Epoch 296, Train Loss: 0.07272355759050697\n",
      "Epoch 297, Train Loss: 0.08872803772101179\n",
      "Epoch 298, Train Loss: 0.2848761789355194\n",
      "Epoch 299, Train Loss: 0.07887215947266668\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "max_len = 100\n",
    "token_to_id = simple_dataset.vocab\n",
    "#print(token_to_id)\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(token_to_id))\n",
    "\n",
    "#dimension_model = 768\n",
    "dimension_model = 16\n",
    "\n",
    "transformer_model = DecoderOnlyTransformerBlockTransformer(num_tokens=len(token_to_id), d_model=dimension_model, max_len=max_len)\n",
    "transformer_model.to(device)\n",
    "optimizer = Adam(transformer_model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in simple_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens, labels = data\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        prediction = transformer_model(input_tokens)\n",
    "        prediction = prediction.view(-1, prediction.size(-1))  # [batch_size * seq_length, num_tokens]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length\n",
    "        loss = criterion(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step(epoch_loss)\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dedc6-2544-4204-99c2-4fadd4fba163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7142eb04-9494-4904-a153-ce883f6e5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is statquest - awesome -  [is, statquest, awesome, ] \n",
      "statquest is - awesome -  [is, statquest, awesome, ] \n",
      "is what awesome - <EOS> -  [awesome, is, <EOS>, ] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_18956\\1332413963.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    }
   ],
   "source": [
    "# Testtexte\n",
    "test_texts = [\n",
    "    \"what is statquest\", #awesome\n",
    "    \"statquest is\", #awesome\n",
    "    \"is what awesome\", #statquest\n",
    "]\n",
    "\n",
    "def string_to_model_input(input_string):\n",
    "    # Split the input string into tokens\n",
    "    tokens = input_string.lower().split()\n",
    "\n",
    "    model_input = []\n",
    "    for token in tokens:\n",
    "        if token in token_to_id:\n",
    "            model_input.append(token_to_id[token])\n",
    "\n",
    "    model_input.append(token_to_id['<EOS>'])\n",
    "    model_input_tensor = torch.tensor(model_input)\n",
    "    return model_input_tensor\n",
    "\n",
    "# Schleife zum Testen des Transformers\n",
    "for text in test_texts:\n",
    "    model_input_expanded = string_to_model_input(text)\n",
    "    model_input_expanded = model_input_expanded.to(device)\n",
    "    model_input_expanded = model_input_expanded.unsqueeze(0)\n",
    "    input_length = model_input_expanded.size(dim=0)\n",
    "    predictions = transformer_model(model_input_expanded)\n",
    "\n",
    "    last_predictions = predictions[-1, :]\n",
    "\n",
    "    max_index = torch.argmax(last_predictions[-1,:])\n",
    "\n",
    "    predicted_id = torch.tensor([max_index], device=device)\n",
    "    predicted_ids = predicted_id\n",
    "\n",
    "    for id in predicted_ids:\n",
    "        topk_values, topk_indices = torch.topk(last_predictions[-1,:], k=3)\n",
    "\n",
    "        # Convert top indices to tokens\n",
    "        predicted_ids = topk_indices\n",
    "        possible_tokens = \" [\"\n",
    "        for id in predicted_ids:\n",
    "            possible_tokens += id_to_token[id.item()] + \", \"\n",
    "        possible_tokens += \"] \"\n",
    "\n",
    "        #print(f\"{id_to_token[id.item()]}\")\n",
    "        print(f\"{text} - {id_to_token[id.item()]} - {possible_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ef9f1-edcb-4622-8fc8-7a5ebce81c09",
   "metadata": {},
   "source": [
    "## load dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "987d9570-c84f-4c51-a96d-6dc172a30e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length: 30\n",
      "created vocab size: 25952\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a: 0\n",
      "treatise: 1\n",
      "of: 2\n",
      "human: 3\n",
      "nature\n",
      "\n",
      "by: 4\n",
      "david: 5\n",
      "hume\n",
      "\n",
      "\n",
      "\n",
      "contents\n",
      "\n",
      "\n",
      ": 6\n",
      ": 7\n",
      "volume: 8\n",
      "i\n",
      "\n",
      ": 9\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 30])\n",
      "25952\n",
      "25952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class Text8Dataset(Dataset):\n",
    "    def __init__(self, file_path, sequence_length):\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            self.text = f.read()\n",
    "        self.min_length = 100\n",
    "        self.text = self.remove_punctuation(self.text)\n",
    "        self.sequence_length = sequence_length\n",
    "        print(f'sequence length: {self.sequence_length}')\n",
    "        self.vocab = self.create_vocabulary(file_path)\n",
    "        print(f'created vocab size: {len(self.vocab)}')\n",
    "        for i, (word, count) in enumerate(self.vocab.items()):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(f'{word}: {count}')\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        words = self.text.split()\n",
    "        self.text_as_int = [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        # Definiere die zu entfernenden Zeichen\n",
    "        punctuation = [',', '\"', \"'\", '.', ';', ':', '!', '?', '_', '“', '‘', '(', ')']\n",
    "\n",
    "        # Ersetze jedes Zeichen in punctuation durch einen leeren String\n",
    "        for char in punctuation:\n",
    "            text = text.replace(char, '')\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_vocabulary(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "        vocab = defaultdict(int)\n",
    "        index = 0\n",
    "\n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "\n",
    "        # Add special tokens\n",
    "        vocab['<EOS>'] = index\n",
    "        index += 1\n",
    "        vocab['<PAD>'] = index\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_as_int) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.text_as_int[idx:idx+self.sequence_length]\n",
    "        target_seq = self.text_as_int[idx+1:idx+self.sequence_length+1]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# Beispiel wie man das Dataset nutzt\n",
    "sequence_length = 30  # Länge der Sequenz\n",
    "#file_path = './datasets/text8.txt'  # Pfad zur Text8-Datei\n",
    "#file_path = './datasets/alice.txt'  # Pfad zur alice-Datei\n",
    "#file_path = \"/content/drive/MyDrive/datasets/alice.txt\"\n",
    "file_path = './datasets/treatise_of_human_nature.txt'\n",
    "\n",
    "dataset = Text8Dataset(file_path, sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Zugriff auf ein Batch\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(input_seq.shape)  # (batch_size, sequence_length)\n",
    "    print(target_seq.shape)  # (batch_size, sequence_length)\n",
    "    break\n",
    "\n",
    "token_to_id = dataset.vocab\n",
    "print(len(token_to_id))\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(id_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3654058-5df7-4264-a845-2312082409fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\AppData\\Local\\Temp\\ipykernel_18956\\1332413963.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_percents = F.softmax(scaled_sims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 3.169320690835112\n",
      "Epoch 1, Train Loss: 1.718541731283035\n",
      "Epoch 2, Train Loss: 1.360911162966526\n",
      "Epoch 3, Train Loss: 1.1702665993187196\n",
      "Epoch 4, Train Loss: 1.044546551551947\n",
      "Epoch 5, Train Loss: 0.9514716159902534\n",
      "Epoch 6, Train Loss: 0.8776419630064011\n",
      "Epoch 7, Train Loss: 0.8175007265200197\n",
      "Epoch 8, Train Loss: 0.7675294698571773\n",
      "Epoch 9, Train Loss: 0.7246098715706064\n",
      "Epoch 10, Train Loss: 0.6870495453690786\n",
      "Epoch 11, Train Loss: 0.6542597012114648\n",
      "Epoch 12, Train Loss: 0.6238061872168741\n",
      "Epoch 13, Train Loss: 0.5953198848558747\n",
      "Epoch 14, Train Loss: 0.5691416561767907\n",
      "Epoch 15, Train Loss: 0.5477537366267458\n",
      "Epoch 16, Train Loss: 0.5275802199676813\n",
      "Epoch 17, Train Loss: 0.5096178783381649\n",
      "Epoch 18, Train Loss: 0.49295114917818106\n",
      "Epoch 19, Train Loss: 0.4786786909846744\n",
      "Epoch 20, Train Loss: 0.46455042940410246\n",
      "Epoch 21, Train Loss: 0.4523689889881764\n",
      "Epoch 22, Train Loss: 0.44088110456610663\n",
      "Epoch 23, Train Loss: 0.43041160476659684\n",
      "Epoch 24, Train Loss: 0.4206221303705522\n",
      "Epoch 25, Train Loss: 0.4117370989435417\n",
      "Epoch 26, Train Loss: 0.4032891931550986\n",
      "Epoch 27, Train Loss: 0.3957898308648803\n",
      "Epoch 28, Train Loss: 0.38868005684679247\n",
      "Epoch 29, Train Loss: 0.3820651776594611\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "max_len = 100\n",
    "token_to_id = dataset.vocab\n",
    "#print(token_to_id)\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "print(len(token_to_id))\n",
    "\n",
    "#dimension_model = 768\n",
    "dimension_model = 256\n",
    "\n",
    "transformer_model = DecoderOnlyTransformerBlockTransformer(num_tokens=len(token_to_id), d_model=dimension_model, max_len=max_len)\n",
    "transformer_model.to(device)\n",
    "optimizer = Adam(transformer_model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens, labels = data\n",
    "        input_tokens = input_tokens.to(device)  # Move inputs to GPU if available\n",
    "        labels = labels.to(device)  # Move labels to GPU if available\n",
    "        # Debugging: Ausgabe der maximalen und minimalen Werte von input_seq\n",
    "        #print(f\"Input Seq - Max Index: {input_seq.max().item()}, Min Index: {input_seq.min().item()}\")\n",
    "        #print(input_tokens.shape)\n",
    "        prediction = transformer_model(input_tokens)\n",
    "        prediction = prediction.view(-1, prediction.size(-1))  # [batch_size * seq_length, num_tokens]\n",
    "        labels = labels.view(-1)  # [batch_size * seq_length\n",
    "        loss = criterion(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_tokens.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step(epoch_loss)\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}, Train Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6c034-23fc-4567-90f6-b358d79f6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testtexte\n",
    "\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy\", #dog\n",
    "    \"She sells seashells by the\", #seashore\n",
    "    \"How much wood would a woodchuck chuck if a woodchuck could chuck\", #wood\n",
    "    \"To be or not to be, that is the\", #question.\n",
    "    \"All that glitters is not\", #gold\n",
    "    \"A journey of a thousand miles begins with a single\", #step\n",
    "    \"Beauty is in the eye of the\", #beholder\n",
    "    \"Actions speak louder than\", #words\n",
    "    \"The early bird catches the\", #worm\n",
    "    \"A picture is worth a thousand\" #words,\n",
    "    \"Once upon a time, in a land far, far away, there lived a brave \", #knight.\n",
    "    \"The stars in the night sky were bright and beautiful, lighting up the \", #darkness.\n",
    "    \"In the middle of the forest, there was a small, hidden cottage made of \", #gingerbread.\n",
    "    \"He who laughs last laughs \", #longest.\n",
    "    \"Every cloud has a silver \", #lining.\n",
    "    \"It's always darkest before the \", #dawn.\n",
    "    \"When the going gets tough, the tough get\", #going.\n",
    "    \"Two heads are better than \", #one.\n",
    "    \"A watched pot never \", #boils.\n",
    "    \"Honesty is the best \", #policy.\n",
    "    \"Alice was not a bit hurt, and she\",\n",
    "    \"Alice opened the door and found that\",\n",
    "    \"After a while, finding that nothing more happened\",\n",
    "    \"Just then her head struck\",\n",
    "    \"As she said this she looked down at her\", #hands\n",
    "    \"won’t talk about cats or\", #hands\n",
    "    \"easy to\"\n",
    "]\n",
    "\n",
    "def string_to_model_input(input_string):\n",
    "    # Split the input string into tokens\n",
    "    tokens = input_string.lower().split()\n",
    "\n",
    "    model_input = []\n",
    "    for token in tokens:\n",
    "        if token in token_to_id:\n",
    "            model_input.append(token_to_id[token])\n",
    "\n",
    "    model_input.append(token_to_id['<EOS>'])\n",
    "    model_input_tensor = torch.tensor(model_input)\n",
    "    return model_input_tensor\n",
    "\n",
    "# Schleife zum Testen des Transformers\n",
    "for text in test_texts:\n",
    "    model_input_expanded = string_to_model_input(text)\n",
    "    model_input_expanded = model_input_expanded.to(device)\n",
    "    model_input_expanded = model_input_expanded.unsqueeze(0)\n",
    "    input_length = model_input_expanded.size(dim=0)\n",
    "    predictions = transformer_model(model_input_expanded)\n",
    "\n",
    "    last_predictions = predictions[-1, :]\n",
    "\n",
    "    max_index = torch.argmax(last_predictions[-1,:])\n",
    "\n",
    "    predicted_id = torch.tensor([max_index], device=device)\n",
    "    predicted_ids = predicted_id\n",
    "\n",
    "    for id in predicted_ids:\n",
    "        topk_values, topk_indices = torch.topk(last_predictions[-1,:], k=5)\n",
    "\n",
    "        # Convert top indices to tokens\n",
    "        predicted_ids = topk_indices\n",
    "        possible_tokens = \" [\"\n",
    "        for id in predicted_ids:\n",
    "            possible_tokens += id_to_token[id.item()] + \", \"\n",
    "        possible_tokens += \"] \"\n",
    "\n",
    "        #print(f\"{id_to_token[id.item()]}\")\n",
    "        print(f\"{text} - {id_to_token[id.item()]} - {possible_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbba02-07a7-4c5c-ac52-a58e4781448e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefaf95d-44e0-41fa-b59f-616baab75ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab3224-ea73-40ed-8736-17d0e13777c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3578c-f665-4d24-89c4-efcb8b6aacb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb818c12-3267-42d1-8ef8-59be79ac9799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a27ae-5a32-46f2-af32-e1d1732d4421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865a1a2-39cc-40ba-81d1-89e54338a82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
